##import libraries 
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from ucimlrepo import fetch_ucirepo 
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.decomposition import PCA 
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,classification_report, confusion_matrix
import time
from scipy import stats
from ucimlrepo import fetch_ucirepo
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.metrics import silhouette_score, adjusted_rand_score
from sklearn.mixture import GaussianMixture
from sklearn.cluster import DBSCAN
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
adult = fetch_ucirepo(id=2) 
# data importing
other_variables = adult.data.features 
income = adult.data.targets 
whole_df=pd.concat([other_variables,income],axis=1)

###FIRST EXERCISE###
#data cleaning.

#we will found possible nan values
nan_counts=other_variables.isnull().sum()
nan_counts
#we find the most frecunece value in each  column
most_frequent_values = {
    "native-country": other_variables["native-country"].mode()[0],#unites-states
    "workclass": other_variables["workclass"].mode()[0]#private
}
print(most_frequent_values)
#now we will change the nan values with the most frequence values that we found above
other_variables["native-country"].fillna(other_variables["native-country"].mode()[0],inplace=True)
other_variables["workclass"].fillna(other_variables["workclass"].mode()[0],inplace=True)
other_variables.dropna(subset=["occupation"], inplace=True)
nan_counts=other_variables.isnull().sum()

#lets check if we have some extra noise
#LETS CHECK COLUMN AGE
other_variables['age'].unique()

#LETS CHECK COLUMN WORKCLASS
other_variables['workclass'].unique()
other_variables.loc[:, "workclass"] = other_variables["workclass"].replace("?", most_frequent_values["workclass"])

#LETS CHECK COLUMN fnlwgt
sorted(other_variables['fnlwgt'].unique())

#LETS CHECK COLUMN education
other_variables['education'].unique()

#LETS CHECK COLUMN education-num
sorted(other_variables['education-num'].unique())

#LETS CHECK COLUMN marital-status
other_variables['marital-status'].unique()

#LETS CHECK COLUMN occupation
other_variables['occupation'].unique()
other_variables = other_variables[other_variables["occupation"] != "?"]

#LETS CHECK COLUMN relationship
other_variables['relationship'].unique()

#LETS CHECK COLUMN race
other_variables['race'].unique()

#LETS CHECK COLUMN sex
other_variables['sex'].unique()

#LETS CHECK COLUMN capital-gain
other_variables['capital-gain'].unique()
zero_count = (other_variables['capital-gain'] == 99999).sum()

#LETS CHECK COLUMN capital-loss
other_variables['capital-loss'].unique()
zero_count = (other_variables['capital-loss'] == 0).sum()

#LETS CHECK COLUMN hours-per-week
other_variables['hours-per-week'].unique()

#LETS CHECK COLUMN native-country
other_variables['native-country'].unique()
other_variables.loc[:, "native-country"] = other_variables["native-country"].replace("?", most_frequent_values["native-country"])

#LETS CHECK COLUMN income
income['income'].unique()
income.loc[:, 'income'] = income['income'].replace({'<=50K.': '<=50K', '>50K.': '>50K'})


#data normalization.
columns_to_normalize = ["age", "fnlwgt", "education-num", "hours-per-week"]
scaler = MinMaxScaler()
other_variables.loc[:, columns_to_normalize] = scaler.fit_transform(other_variables[columns_to_normalize])

columns_to_log_scale = ["capital-gain", "capital-loss"]
for col in columns_to_log_scale:
    other_variables.loc[:, col] = np.log1p(other_variables[col])


categorical_columns = ['workclass', 'education', 'marital-status', 'occupation', 
                       'relationship', 'race', 'sex', 'native-country']
label_encoders = {}
for col in categorical_columns:
    le = LabelEncoder()
    other_variables.loc[:, col] = le.fit_transform(other_variables[col]) 
    label_encoders[col] = le  
encoding_mappings = {col: {category: i for i, category in enumerate(label_encoders[col].classes_)} 
                     for col in categorical_columns}

encoding_df = pd.DataFrame.from_dict(encoding_mappings, orient="index").transpose()
income_encoder = LabelEncoder()
income_encoded = income_encoder.fit_transform(income.values.ravel()) 
income = pd.DataFrame(income_encoded, columns=['income']) 
label_encoders['income'] = income_encoder

#data reduction.

# make 2 datasets have the same index
other_variables = other_variables.reset_index(drop=True)
income = income.reset_index(drop=True)
common_index = other_variables.index.intersection(income.index)
# keep common rows
other_variables = other_variables.loc[common_index].reset_index(drop=True)
income = income.loc[common_index].reset_index(drop=True)
# PCA while keeping 95% of the variance
pca = PCA(n_components=0.95) 
pct = pca.fit_transform(other_variables)
num_components = pca.n_components_
pca_columns = [f'pc{i+1}' for i in range(num_components)]
principal_df = pd.DataFrame(pct, columns=pca_columns)
finaldf = pd.concat([principal_df, income], axis=1)
print(f"Number of selected principal components: {num_components}")
print(finaldf)

###SECOND EXERCISE###
#Future Selection.
#Feature Selection using SelectKBest
features = other_variables
target = income.values.ravel()
selector = SelectKBest(score_func=f_classif, k=5) 
features_new = selector.fit_transform(features, target)
selected_features_kbest = features.columns[selector.get_support()]
selected_features_df = pd.DataFrame({
    "Method": ["SelectKBest"] * 5 ,
    "Feature": selected_features_kbest.tolist() 
})
print(selected_features_df)

#Training Models Full and with 5 Features

features_full = other_variables
features_reduced = other_variables[selected_features_kbest]
target= income
X_train_full, X_test_full, y_train, y_test = train_test_split(features_full, target, test_size=0.2, random_state=42)
X_train_reduced, X_test_reduced, y_train_reduced, y_test_reduced = train_test_split(features_reduced, target, test_size=0.2, random_state=42)

# Training Decision Tree  Full
start_time = time.time()
dt_full = DecisionTreeClassifier(random_state=42)
dt_full.fit(X_train_full, y_train)
y_pred_full = dt_full.predict(X_test_full)
accuracy_full = accuracy_score(y_test, y_pred_full)
time_full = time.time() - start_time

# Decision Tree 5 Variables
start_time = time.time()
dt_reduced = DecisionTreeClassifier(random_state=42)
dt_reduced.fit(X_train_reduced, y_train_reduced)
y_pred_reduced = dt_reduced.predict(X_test_reduced)
accuracy_reduced = accuracy_score(y_test_reduced, y_pred_reduced)
time_reduced = time.time() - start_time

# Print Results
comparison_df = pd.DataFrame({
    "Model": ["Full Feature ", "Reduced Feature"],
    "Accuracy": [accuracy_full, accuracy_reduced],
    "Training Time (seconds)": [time_full, time_reduced]
})
print(comparison_df)



#Managing Outliers.

#detect outliers with z score
selected_columns = ["age", "capital-gain", "capital-loss", "hours-per-week","fnlwgt"]
z_scores = np.abs(stats.zscore(whole_df[selected_columns]))
threshold = 3.5
outliers = (z_scores > threshold).sum(axis=0)
outliers_summary_zscore = pd.DataFrame({
    "Column": selected_columns,
    "Number of Outliers": outliers
})
print(outliers_summary_zscore)

#plot out outliers - box plots
def plot_outliers_boxplots(data, selected_columns):
    for column in selected_columns:
        plt.figure(figsize=(8, 5))
        sns.boxplot(x=data[column])
        plt.title(f"Boxplot of {column}")
        plt.show()
plot_outliers_boxplots(other_variables, selected_columns)

#correlation matrix plot
correlation_matrix = whole_df.corr()
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm", linewidths=0.5)
plt.title("Correlation Matrix of Features")
plt.show()


#we take out fnlwgt
other_variables = other_variables.drop(columns=["fnlwgt"])
#for age and hours per week we will cap extreme values at the 99th precentile 
for col in ["age", "hours-per-week"]:
    upper_limit = other_variables[col].quantile(0.99)
    other_variables[col] = np.where(other_variables[col] > upper_limit, upper_limit, other_variables[col]) 
# for capital gain and capital loss ,we already have logarithm used we will cap them too in 5 and 99 percentile
for col in ["capital-gain", "capital-loss"]:
    lower_limit = other_variables[col].quantile(0.05) 
    upper_limit = other_variables[col].quantile(0.95) 
    other_variables[col] = np.clip(other_variables[col], lower_limit, upper_limit) 
    
selected_columns = ["age", "capital-gain", "capital-loss", "hours-per-week"]
z_scores = np.abs(stats.zscore(other_variables[selected_columns]))
threshold = 3.5
outliers = (z_scores > threshold).sum(axis=0)
outliers_summary_zscore = pd.DataFrame({
    "Column": selected_columns,
    "Number of Outliers": outliers
})
print(outliers_summary_zscore)

###THIRD EXERCISE###
k = 2
X_clustering=other_variables.copy()
#K-Means
kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
kmeans_labels = kmeans.fit_predict(X_clustering)
silhouette_kmeans = silhouette_score(X_clustering, kmeans_labels)
#Agglomerative Clustering
agglo = AgglomerativeClustering(n_clusters=k)
agglo_labels = agglo.fit_predict(X_clustering)
silhouette_agglo = silhouette_score(X_clustering, agglo_labels)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_clustering)
fig, axes = plt.subplots(1, 2, figsize=(14, 6))
# Scatter plot  K-Means
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=kmeans_labels, palette="viridis", ax=axes[0], s=10)
axes[0].set_title(f"K-Means Clustering Î¼Îµ k={k}\nSilhouette Score: {silhouette_kmeans:.4f}")
axes[0].set_xlabel("Principal Component 1")
axes[0].set_ylabel("Principal Component 2")
axes[0].legend(title="Clusters")
# Scatter plot Agglomerative
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=agglo_labels, palette="magma", ax=axes[1], s=10)
axes[1].set_title(f"Agglomerative Clustering Î¼Îµ k={k}\nSilhouette Score: {silhouette_agglo:.4f}")
axes[1].set_xlabel("Principal Component 1")
axes[1].set_ylabel("Principal Component 2")
axes[1].legend(title="Clusters")
plt.tight_layout()
plt.show()
silhouette_scores = pd.DataFrame({
    "Algorithm": ["K-Means", "Agglomerative"],
    "Silhouette Score": [silhouette_kmeans, silhouette_agglo]
})
print(silhouette_scores)

#we will try for k=3 and for k=4 to see what is happening
cluster_numbers = [3, 4]
silhouette_scores3_4 = {"Algorithm": [], "Clusters": [], "Silhouette Score": []}
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_clustering)
fig, axes = plt.subplots(len(cluster_numbers), 2, figsize=(14, 6 * len(cluster_numbers)))

for idx, k in enumerate(cluster_numbers):
    #K-Means
    kmeans3_4 = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans_labels3_4 = kmeans3_4.fit_predict(X_clustering)
    silhouette_kmeans3_4 = silhouette_scores3_4(X_clustering, kmeans_labels3_4)

    #Agglomerative Clustering
    agglo = AgglomerativeClustering(n_clusters=k)
    agglo_labels = agglo.fit_predict(X_clustering)
    silhouette_agglo = silhouette_score(X_clustering, agglo_labels)
    silhouette_scores["Algorithm"].extend(["K-Means", "Agglomerative"])
    silhouette_scores["Clusters"].extend([k, k])
    silhouette_scores["Silhouette Score"].extend([silhouette_kmeans, silhouette_agglo])

    #Scatter plot  K-Means
    sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=kmeans_labels, palette="viridis", ax=axes[idx, 0], s=10)
    axes[idx, 0].set_title(f"K-Means Clustering Î¼Îµ k={k}\nSilhouette Score: {silhouette_kmeans:.4f}")
    axes[idx, 0].set_xlabel("Principal Component 1")
    axes[idx, 0].set_ylabel("Principal Component 2")
    axes[idx, 0].legend(title="Clusters")

    #Scatter plot Agglomerative Clustering
    sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=agglo_labels, palette="magma", ax=axes[idx, 1], s=10)
    axes[idx, 1].set_title(f"Agglomerative Clustering Î¼Îµ k={k}\nSilhouette Score: {silhouette_agglo:.4f}")
    axes[idx, 1].set_xlabel("Principal Component 1")
    axes[idx, 1].set_ylabel("Principal Component 2")
    axes[idx, 1].legend(title="Clusters")

plt.tight_layout()
plt.show()

results_df = pd.DataFrame(silhouette_scores)
print(results_df)
###FORTH EXERCISE###
X = other_variables.copy()  # Features
y = income.values.ravel() # Target variable
pd.Series(y).value_counts()
# Split dataset into 80% training and 20% testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
models = {
    "Logistic Regression": LogisticRegression(max_iter=500, class_weight='balanced'),
    "Random Forest": RandomForestClassifier(n_estimators=300, random_state=42, class_weight='balanced'),
    "Support Vector Machine": SVC(class_weight='balanced'),
    "Gradient Boosting": GradientBoostingClassifier(n_estimators=100, random_state=42)
}

results = []

for model_name, model in models.items():
    # Train model
    model.fit(X_train, y_train)
    
    # Predict on test set
    y_pred = model.predict(X_test)
    
    # Evaluate results
    accuracy = accuracy_score(y_test, y_pred)
    precision = classification_report(y_test, y_pred, output_dict=True)["weighted avg"]["precision"]
    recall = classification_report(y_test, y_pred, output_dict=True)["weighted avg"]["recall"]
    f1 = classification_report(y_test, y_pred, output_dict=True)["weighted avg"]["f1-score"]
    
    # Store results
    results.append({"Model": model_name, "Accuracy": accuracy, "Precision": precision, "Recall": recall, "F1-Score": f1})
    
    # Plot confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(5,4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=["<=50K", ">50K"], yticklabels=["<=50K", ">50K"])
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title(f"Confusion Matrix - {model_name}")
    plt.show()

# 3ï¸âƒ£ Display Results
machine_learning_results_df = pd.DataFrame(results)
print(machine_learning_results_df)

# ÎŸÎ¹ confusion matrices Î´ÎµÎ¯Ï‡Î½Î¿Ï…Î½ Ï„Î·Î½ Î±Ï€ÏŒÎ´Î¿ÏƒÎ· ÎºÎ¬Î¸Îµ Î¼Î¿Î½Ï„Î­Î»Î¿Ï… ÏƒÎµ Î´ÏÎ¿ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯ÎµÏ‚:

# <=50K (Î§Î±Î¼Î·Î»ÏŒ Î•Î¹ÏƒÏŒÎ´Î·Î¼Î±)
# >50K (Î¥ÏˆÎ·Î»ÏŒ Î•Î¹ÏƒÏŒÎ´Î·Î¼Î±)
# ğŸ”¹ Logistic Regression
# True Positives (TP) = 2934
# False Negatives (FN) = 0 (Î¬ÏÎ± Î´ÎµÎ½ Ï€ÏÎ¿Î²Î»Î­Ï€ÎµÎ¹ ÏƒÏ‡ÎµÎ´ÏŒÎ½ ÎºÎ±Î¸ÏŒÎ»Î¿Ï… ÏƒÏ‰ÏƒÏ„Î¬ Ï„Î¿ >50K)
# False Positives (FP) = 2934 (Ï€Î¿Î»Î»Î¬ Î»Î¬Î¸Î¿Ï‚ predictions Î³Î¹Î± <=50K)
# Accuracy = 54.2% (Ï‡Î±Î¼Î·Î»ÏŒ)
# ğŸ“Œ Î£Ï…Î¼Ï€Î­ÏÎ±ÏƒÎ¼Î±: Î¤Î¿ Logistic Regression Î´ÎµÎ½ Î±Ï€Î¿Î´Î¯Î´ÎµÎ¹ ÎºÎ±Î»Î¬, ÎºÎ±Î¸ÏÏ‚ Î¼Ï€ÎµÏÎ´ÎµÏÎµÎ¹ Î±ÏÎºÎµÏ„Î¬ Ï„Î¹Ï‚ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯ÎµÏ‚ ÎºÎ±Î¹ Î­Ï‡ÎµÎ¹ Ï‡Î±Î¼Î·Î»ÏŒ Accuracy.

# ğŸ”¹ Random Forest
# True Positives (TP) = 809 (Î»Î¯Î³ÎµÏ‚ ÏƒÏ‰ÏƒÏ„Î­Ï‚ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ >50K)
# False Negatives (FN) = 0 (Î±ÎºÏŒÎ¼Î± ÏƒÎ·Î¼Î±Î½Ï„Î¹ÎºÏŒ Ï€ÏÏŒÎ²Î»Î·Î¼Î±)
# Accuracy = 69.7% (Î¼Î­Ï„ÏÎ¹Î¿)
# ğŸ“Œ Î£Ï…Î¼Ï€Î­ÏÎ±ÏƒÎ¼Î±: Î¤Î¿ Random Forest ÎµÎ¯Î½Î±Î¹ ÏƒÎ·Î¼Î±Î½Ï„Î¹ÎºÎ¬ ÎºÎ±Î»ÏÏ„ÎµÏÎ¿ Î±Ï€ÏŒ Ï„Î¿ Logistic Regression, Î±Î»Î»Î¬ Î´Ï…ÏƒÎºÎ¿Î»ÎµÏÎµÏ„Î±Î¹ Î±ÎºÏŒÎ¼Î± Î½Î± Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹ Ï„Î± Î¬Ï„Î¿Î¼Î± Î¼Îµ Ï…ÏˆÎ·Î»ÏŒ ÎµÎ¹ÏƒÏŒÎ´Î·Î¼Î±.

# ğŸ”¹ Support Vector Machine (SVM)
# True Positives (TP) = 1693 (ÎºÎ±Î»ÏÏ„ÎµÏÎ· Î±Ï€ÏŒÎ´Î¿ÏƒÎ· ÏƒÏ„Î¿ >50K)
# False Negatives (FN) = 0 (ÎºÎ±Î¼Î¯Î± ÏƒÏ‰ÏƒÏ„Î® Ï€ÏÏŒÎ²Î»ÎµÏˆÎ· Î³Î¹Î± >50K)
# Accuracy = 63.5% (Î¼Î­Ï„ÏÎ¹Î¿)
# ğŸ“Œ Î£Ï…Î¼Ï€Î­ÏÎ±ÏƒÎ¼Î±: Î¤Î¿ SVM Î±Ï€Î¿Î´Î¯Î´ÎµÎ¹ Î»Î¯Î³Î¿ ÎºÎ±Î»ÏÏ„ÎµÏÎ± Î±Ï€ÏŒ Ï„Î¿ Logistic Regression, Î±Î»Î»Î¬ Î´ÎµÎ½ Î´Î¹Î±Ï‡Ï‰ÏÎ¯Î¶ÎµÎ¹ ÎºÎ±Î»Î¬ Ï„Î¹Ï‚ Î´ÏÎ¿ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯ÎµÏ‚.

# ğŸ”¹ Gradient Boosting
# True Positives (TP) = 2 (Î¿Ï…ÏƒÎ¹Î±ÏƒÏ„Î¹ÎºÎ¬ Î±Ï€Î¿Ï„Ï…Î³Ï‡Î¬Î½ÎµÎ¹ Î½Î± Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹ Ï„Î·Î½ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î± >50K)
# False Negatives (FN) = 0
# Accuracy = 75.7% (Ï…ÏˆÎ·Î»ÏŒ)
# ğŸ“Œ Î£Ï…Î¼Ï€Î­ÏÎ±ÏƒÎ¼Î±: Î Î±ÏÏŒÎ»Î¿ Ï€Î¿Ï… Î­Ï‡ÎµÎ¹ Ï„Î¿ Ï…ÏˆÎ·Î»ÏŒÏ„ÎµÏÎ¿ Accuracy, Î´ÎµÎ½ Ï€ÏÎ¿Î²Î»Î­Ï€ÎµÎ¹ ÏƒÏ‡ÎµÎ´ÏŒÎ½ ÎºÎ±Î¸ÏŒÎ»Î¿Ï… ÏƒÏ‰ÏƒÏ„Î¬ Ï„Î·Î½ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î± >50K, Ï€Î¿Ï… ÎµÎ¯Î½Î±Î¹ Î¼ÎµÎ³Î¬Î»Î¿ Ï€ÏÏŒÎ²Î»Î·Î¼Î±


from imblearn.over_sampling import SMOTE
import sklearn
import imblearn

# Î•Ï†Î±ÏÎ¼Î¿Î³Î® SMOTE Î³Î¹Î± Ï„Î·Î½ ÎµÎ¾Î¹ÏƒÎ¿ÏÏÏŒÏ€Î·ÏƒÎ· Ï„Ï‰Î½ ÎºÎ»Î¬ÏƒÎµÏ‰Î½
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# Î•Ï€Î¹Î²ÎµÎ²Î±Î¯Ï‰ÏƒÎ· Ï„Î·Ï‚ Î½Î­Î±Ï‚ ÎºÎ±Ï„Î±Î½Î¿Î¼Î®Ï‚ Ï„Ï‰Î½ ÎºÎ»Î¬ÏƒÎµÏ‰Î½
unique, counts = np.unique(y_resampled, return_counts=True)
balanced_class_distribution = dict(zip(unique, counts))

# Î•Ï€Î±Î½ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· Ï„Ï‰Î½ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ Î¼Îµ Ï„Î± ÎµÎ¾Î¹ÏƒÎ¿ÏÏÎ¿Ï€Î·Î¼Î­Î½Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î±
new_results = []

for model_name, model in models.items():
    # Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· Î¼Îµ Ï„Î± Î½Î­Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î±
    model.fit(X_resampled, y_resampled)
    
    # Î ÏÏŒÎ²Î»ÎµÏˆÎ· ÏƒÏ„Î¿ test set
    y_pred_smote = model.predict(X_test)
    
    # Î‘Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ· Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½
    accuracy = accuracy_score(y_test, y_pred_smote)
    precision = classification_report(y_test, y_pred_smote, output_dict=True)["weighted avg"]["precision"]
    recall = classification_report(y_test, y_pred_smote, output_dict=True)["weighted avg"]["recall"]
    f1 = classification_report(y_test, y_pred_smote, output_dict=True)["weighted avg"]["f1-score"]
    
    # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½
    new_results.append({"Model": model_name, "Accuracy": accuracy, "Precision": precision, "Recall": recall, "F1-Score": f1})
    
    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred_smote)
    plt.figure(figsize=(5,4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=["<=50K", ">50K"], yticklabels=["<=50K", ">50K"])
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title(f"Confusion Matrix - {model_name} (SMOTE Applied)")
    plt.show()

# Î ÏÎ¿Î²Î¿Î»Î® Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½ Î¼ÎµÏ„Î¬ Ï„Î·Î½ ÎµÏ†Î±ÏÎ¼Î¿Î³Î® SMOTE
new_machine_learing_results_df = pd.DataFrame(new_results)
print(new_machine_learing_results_df)

# 1ï¸âƒ£ Logistic Regression (Î Î±Î»Î¹Î½Î´ÏÏŒÎ¼Î·ÏƒÎ·)
# ğŸ“Š Confusion Matrix

# 3721 ÏƒÏ‰ÏƒÏ„Î­Ï‚ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ Î³Î¹Î± <=50K (True Negatives)
# 3252 Î»Î±Î½Î¸Î±ÏƒÎ¼Î­Î½ÎµÏ‚ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ ÏŒÏ€Î¿Ï… <=50K Ï€ÏÎ¿Î²Î»Î­Ï†Î¸Î·ÎºÎ±Î½ Ï‰Ï‚ >50K
# 0 ÏƒÏ‰ÏƒÏ„Î­Ï‚ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ Î³Î¹Î± >50K (False Negatives)
# Î— Î±ÎºÏÎ¯Î²ÎµÎ¹Î± Î­Ï€ÎµÏƒÎµ ÏƒÏ„Î¿ 51.9%
# ğŸ” Î‘Î½Î¬Î»Ï…ÏƒÎ·

# ÎŸ Ï„Î±Î¾Î¹Î½Î¿Î¼Î·Ï„Î®Ï‚ ÎºÎ¬Î½ÎµÎ¹ Ï€Î¬ÏÎ± Ï€Î¿Î»Î»Î¬ Î»Î¬Î¸Î· ÏƒÏ„Î·Î½ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î± >50K.
# Î Î¹Î¸Î±Î½ÏŒÏ„Î±Ï„Î± Î´ÎµÎ½ Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Ï€ÏÎ¿ÏƒÎ±ÏÎ¼Î¿ÏƒÏ„ÎµÎ¯ ÏƒÏ‰ÏƒÏ„Î¬ ÏƒÏ„Î± Î½Î­Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Ï€Î¿Ï… Î´Î·Î¼Î¹Î¿ÏÏÎ³Î·ÏƒÎµ Ï„Î¿ SMOTE.
# Î“ÎµÎ½Î¹ÎºÎ¬, Î· Logistic Regression Î´ÎµÎ½ ÎµÎ¯Î½Î±Î¹ Ï„ÏŒÏƒÎ¿ Î¹ÏƒÏ‡Ï…ÏÏŒÏ‚ Ï„Î±Î¾Î¹Î½Î¿Î¼Î·Ï„Î®Ï‚ Î³Î¹Î± Ï€ÎµÏÎ¯Ï€Î»Î¿ÎºÎ± Ï€ÏÎ¿Î²Î»Î®Î¼Î±Ï„Î±.
# ğŸ›  Î¤Î¹ Î¼Ï€Î¿ÏÎ¿ÏÎ¼Îµ Î½Î± ÎºÎ¬Î½Î¿Ï…Î¼Îµ;

# Î”Î¿ÎºÎ¹Î¼Î® Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÏÎ½ ÎºÎ±Î½Î¿Î½Î¹ÎºÎ¿Ï€Î¿Î¹Î®ÏƒÎµÏ‰Î½ Î® Î´Î¹Î±Î³ÏÎ±Ï†Î® Ï€ÎµÏÎ¹Ï„Ï„ÏÎ½ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½.
# 2ï¸âƒ£ Random Forest
# ğŸ“Š Confusion Matrix

# 5542 ÏƒÏ‰ÏƒÏ„Î­Ï‚ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ Î³Î¹Î± <=50K
# 1431 Î»Î±Î½Î¸Î±ÏƒÎ¼Î­Î½ÎµÏ‚ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ Î³Î¹Î± <=50K
# Î— Î±ÎºÏÎ¯Î²ÎµÎ¹Î± Î²ÎµÎ»Ï„Î¹ÏÎ¸Î·ÎºÎµ ÏƒÏ„Î¿ 65.4% (ÏƒÏ…Î³ÎºÏÎ¹Ï„Î¹ÎºÎ¬ Î¼Îµ Ï€ÏÎ¹Î½)
# ğŸ” Î‘Î½Î¬Î»Ï…ÏƒÎ·

# Î¤Î¿ Random Forest Î±Î½Ï„Î±Ï€Î¿ÎºÏÎ¯Î¸Î·ÎºÎµ Ï€Î¿Î»Ï ÎºÎ±Î»ÏÏ„ÎµÏÎ± Î±Ï€ÏŒ Ï„Î· Logistic Regression.
# ÎˆÏ‡ÎµÎ¹ Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ· Î¹ÏƒÎ¿ÏÏÎ¿Ï€Î¯Î± ÏƒÏ„Î¹Ï‚ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚, Î±Î»Î»Î¬ Î±ÎºÏŒÎ¼Î± Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î»Î¬Î¸Î·.
# ğŸ›  Î¤Î¹ Î¼Ï€Î¿ÏÎ¿ÏÎ¼Îµ Î½Î± ÎºÎ¬Î½Î¿Ï…Î¼Îµ;

# Î‘ÏÎ¾Î·ÏƒÎ· Ï„Î¿Ï… Î±ÏÎ¹Î¸Î¼Î¿Ï Ï„Ï‰Î½ Î´Î­Î½Ï„ÏÏ‰Î½ (n_estimators).
# Î”Î¿ÎºÎ¹Î¼Î® Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÏÎ½ Ï…Ï€ÎµÏÏ€Î±ÏÎ±Î¼Î­Ï„ÏÏ‰Î½ (max_depth, min_samples_split, ÎºÏ„Î».).
# 3ï¸âƒ£ Support Vector Machine
# ğŸ“Š Confusion Matrix

# 4227 ÏƒÏ‰ÏƒÏ„Î­Ï‚ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ Î³Î¹Î± <=50K
# 2746 Î»Î±Î½Î¸Î±ÏƒÎ¼Î­Î½ÎµÏ‚ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ Î³Î¹Î± <=50K
# Î‘ÎºÏÎ¯Î²ÎµÎ¹Î± ÏƒÏ„Î¿ 55.2%
# ğŸ” Î‘Î½Î¬Î»Ï…ÏƒÎ·

# Î‘ÎºÏŒÎ¼Î± Ï€Î¿Î»Î»Î¬ Î»Î¬Î¸Î· ÎºÎ±Î¹ ÏƒÏ„Î¹Ï‚ Î´ÏÎ¿ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯ÎµÏ‚.
# ÎŸ Î±Î»Î³ÏŒÏÎ¹Î¸Î¼Î¿Ï‚ Î´ÎµÎ½ Î±Î½Ï„Î±Ï€Î¿ÎºÏÎ¯Î¸Î·ÎºÎµ ÎºÎ±Î»Î¬ ÏƒÏ„Î¿ SMOTE.
# ğŸ›  Î¤Î¹ Î¼Ï€Î¿ÏÎ¿ÏÎ¼Îµ Î½Î± ÎºÎ¬Î½Î¿Ï…Î¼Îµ;

# Î”Î¿ÎºÎ¹Î¼Î® Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÏÎ½ kernels (RBF, polynomial).
# Î¡ÏÎ¸Î¼Î¹ÏƒÎ· Ï„Î·Ï‚ Ï€Î±ÏÎ±Î¼Î­Ï„ÏÎ¿Ï… C.
# 4ï¸âƒ£ Gradient Boosting
# ğŸ“Š Confusion Matrix

# 6362 ÏƒÏ‰ÏƒÏ„Î­Ï‚ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ Î³Î¹Î± <=50K
# 611 Î»Î±Î½Î¸Î±ÏƒÎ¼Î­Î½ÎµÏ‚ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ Î³Î¹Î± <=50K
# Î— ÎºÎ±Î»ÏÏ„ÎµÏÎ· Î±ÎºÏÎ¯Î²ÎµÎ¹Î± ÏƒÏ„Î¿ 71.1%!
# ğŸ” Î‘Î½Î¬Î»Ï…ÏƒÎ·

# Î¤Î¿ Gradient Boosting Î±Ï€Î¿Î´Î¯Î´ÎµÎ¹ Ï„Î¿ ÎºÎ±Î»ÏÏ„ÎµÏÎ¿ Î±Ï€Î¿Ï„Î­Î»ÎµÏƒÎ¼Î±.
# ÎˆÏ‡ÎµÎ¹ Ï„Î·Î½ Ï…ÏˆÎ·Î»ÏŒÏ„ÎµÏÎ· Î±ÎºÏÎ¯Î²ÎµÎ¹Î±, recall ÎºÎ±Î¹ F1-score.
# ğŸ›  Î¤Î¹ Î¼Ï€Î¿ÏÎ¿ÏÎ¼Îµ Î½Î± ÎºÎ¬Î½Î¿Ï…Î¼Îµ;

# Î‘ÏÎ¾Î·ÏƒÎ· Ï„Ï‰Î½ n_estimators.
# Î”Î¿ÎºÎ¹Î¼Î® Learning Rate (Î¼ÎµÎ¯Ï‰ÏƒÎ· Î³Î¹Î± ÎºÎ±Î»ÏÏ„ÎµÏÎ· Î³ÎµÎ½Î¯ÎºÎµÏ…ÏƒÎ·)
####Î²Î»ÎµÏ‚Ï€Î¿Ï…Î¼Îµ Î¿Ï„Î¹ Î¿Ï…Ï„Îµ Î¼Îµ Ï„Î·Î½ ÎµÎ¾ÏƒÎ¹ÏƒÎ¿ÏÎ¿Ï€Î·ÏƒÎ· Î´ÎµÎ´Î¿Î¼ÎµÎ½Ï‰Î½ Î²Î³Î±Î¶Î¿Ï…Î¼Îµ ÎºÎ±ÏÎ· Î¿Ï€Î¿Ï„Îµ Ï€Î±Î¼Îµ Î½Î± Î´Î¿ÎºÎ¹Î¼Î±ÏƒÎ¿Ï…Î¼Îµ Î¼Î¿Î½Î¿ Ï„Î¹Ï‚ 5 Î¼ÎµÏ„Î±Î²Î»Î·Ï„ÎµÏ‚ Î±Ï€Î¿ Ï„Î¿ select k best
selected_features_kbest = ["education-num", "marital-status", "race", "capital-gain", "native-country"]
X_selected = other_variables[selected_features_kbest]
X_train, X_test, y_train, y_test = train_test_split(X_selected, income.values.ravel(), test_size=0.2, random_state=42)

models = {
    "Logistic Regression": LogisticRegression(max_iter=500),
    "Random Forest": RandomForestClassifier(n_estimators=300, random_state=42),
    "Support Vector Machine": SVC(),
    "Gradient Boosting": GradientBoostingClassifier(n_estimators=100, random_state=42)
}

results = []

for model_name, model in models.items():
    # Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· Î¼Îµ Ï„Î± Î½Î­Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î±
    model.fit(X_train, y_train)
    
    # Î ÏÏŒÎ²Î»ÎµÏˆÎ· ÏƒÏ„Î¿ test set
    y_pred = model.predict(X_test)
    
    # Î‘Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ· Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½
    accuracy = accuracy_score(y_test, y_pred)
    precision = classification_report(y_test, y_pred, output_dict=True)["weighted avg"]["precision"]
    recall = classification_report(y_test, y_pred, output_dict=True)["weighted avg"]["recall"]
    f1 = classification_report(y_test, y_pred, output_dict=True)["weighted avg"]["f1-score"]
    
    # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½
    results.append({"Model": model_name, "Accuracy": accuracy, "Precision": precision, "Recall": recall, "F1-Score": f1})
    
    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(5,4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=["<=50K", ">50K"], yticklabels=["<=50K", ">50K"])
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title(f"Confusion Matrix - {model_name} (5best)")
    plt.show()

# Î ÏÎ¿Î²Î¿Î»Î® Ï„Ï‰Î½ Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½
results_df_5best = pd.DataFrame(results)
print(results_df_5best)
#oute edw vriskoume kapoio montelo pou na einai deleastiko
# 1ï¸âƒ£ Feature Selection with SelectKBest
# You used SelectKBest to reduce the number of features to the top 5 most relevant based on their correlation with the target.
# The models trained on these selected features show an accuracy comparable to those trained on the full dataset.
# However, the confusion matrices indicate that some models perform extremely poorly in predicting the ">50K" class.
# 2ï¸âƒ£ Confusion Matrices (5 Best Features)
# Logistic Regression, SVM: These models completely fail to predict the ">50K" class (no values in the true positive quadrant).
# Random Forest & Gradient Boosting: Show minor improvements in detecting the ">50K" class, but the number of false negatives remains high.
# ğŸ”´ Interpretation:

# The drastic class imbalance is likely causing these models to be biased towards predicting only "<=50K".
# The removal of important features might have stripped away crucial information needed to separate the two income classes effectively.
#  Observations:

# Accuracy is not significantly affected by the feature reduction.
# Gradient Boosting shows the highest Precision (0.677), indicating it is more confident in its positive predictions.
# Random Forest shows similar performance to the full-feature model, meaning it still retains good predictive power.

#ÏƒÏ…Î½Î¿Î»Î¹ÎºÎ¿ ÏƒÏ…Î¼Ï€ÎµÏÎ±ÏƒÎ¼Î± 
# ÎœÎµ Î²Î¬ÏƒÎ· Ï„Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± Ï„Ï‰Î½ Ï„ÏÎ¹ÏÎ½ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÏÎ½ Ï€ÏÎ¿ÏƒÎµÎ³Î³Î¯ÏƒÎµÏ‰Î½ (Î±ÏÏ‡Î¹ÎºÎ¬ Î´ÎµÎ´Î¿Î¼Î­Î½Î±, ÎµÎ¾Î¹ÏƒÎ¿ÏÏÎ¿Ï€Î·Î¼Î­Î½Î± Î¼Îµ SMOTE, ÎºÎ±Î¹ Ï‡ÏÎ®ÏƒÎ· Ï„Ï‰Î½ 5 ÎºÎ±Î»ÏÏ„ÎµÏÏ‰Î½ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½), Î· ÎºÎ±Î»ÏÏ„ÎµÏÎ· ÎµÏ€Î¹Î»Î¿Î³Î® ÎµÎ¾Î±ÏÏ„Î¬Ï„Î±Î¹ Î±Ï€ÏŒ Ï„Î·Î½ Ï€ÏÎ¿Ï„ÎµÏÎ±Î¹ÏŒÏ„Î·Ï„Î¬ ÏƒÎ¿Ï….

# Î‘Î½ Î´Î¯Î½Î¿Ï…Î¼Îµ Î­Î¼Ï†Î±ÏƒÎ· ÏƒÏ„Î·Î½ Î±ÎºÏÎ¯Î²ÎµÎ¹Î± (Accuracy):

# Gradient Boosting ÏƒÏ„Î¹Ï‚ Î±ÏÏ‡Î¹ÎºÎ­Ï‚ ÏƒÏ…Î½Î¸Î®ÎºÎµÏ‚ ÎºÎ±Î¹ Î¼Îµ Ï„Î± 5 ÎºÎ±Î»ÏÏ„ÎµÏÎ± Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ Î­Ï‡ÎµÎ¹ Ï„Î·Î½ Ï…ÏˆÎ·Î»ÏŒÏ„ÎµÏÎ· Î±ÎºÏÎ¯Î²ÎµÎ¹Î± (0.757).
# Random Forest ÎµÏ€Î¯ÏƒÎ·Ï‚ Î­Ï‡ÎµÎ¹ ÎºÎ±Î»Î® Î±ÎºÏÎ¯Î²ÎµÎ¹Î±, Î±Î»Î»Î¬ Ï‡Î±Î¼Î·Î»ÏŒÏ„ÎµÏÎ· Î±Ï€ÏŒ Ï„Î¿ Gradient Boosting.
# Î‘Î½ Î´Î¯Î½Î¿Ï…Î¼Îµ Î­Î¼Ï†Î±ÏƒÎ· ÏƒÏ„Î·Î½ Î±Î½Î¬ÎºÎ»Î·ÏƒÎ· (Recall) Î³Î¹Î± Ï„Î·Î½ Î±Î½Î¯Ï‡Î½ÎµÏ…ÏƒÎ· Ï„Ï‰Î½ Ï…ÏˆÎ·Î»ÏÎ½ ÎµÎ¹ÏƒÎ¿Î´Î·Î¼Î¬Ï„Ï‰Î½ (>50K):

# ÎœÎµÏ„Î¬ Ï„Î·Î½ ÎµÏ†Î±ÏÎ¼Î¿Î³Î® Ï„Î¿Ï… SMOTE, Ï„Î¿ Logistic Regression ÎºÎ±Î¹ Ï„Î¿ Random Forest Î­Ï‡Î¿Ï…Î½ ÎºÎ±Î»ÏÏ„ÎµÏÎ· Î¹ÏƒÎ¿ÏÏÎ¿Ï€Î¯Î± recall, Î±Î»Î»Î¬ Î¼ÎµÎ¹Ï‰Î¼Î­Î½Î· Î±ÎºÏÎ¯Î²ÎµÎ¹Î±.
# Î‘Î½ Î´Î¯Î½Î¿Ï…Î¼Îµ Î­Î¼Ï†Î±ÏƒÎ· ÏƒÏ„Î·Î½ Î¹ÏƒÎ¿ÏÏÎ¿Ï€Î¯Î± Î¼ÎµÏ„Î±Î¾Ï Î±ÎºÏÎ¯Î²ÎµÎ¹Î±Ï‚ ÎºÎ±Î¹ Î±Î½Î¬ÎºÎ»Î·ÏƒÎ·Ï‚ (F1-score):

# Gradient Boosting Î´Î¹Î±Ï„Î·ÏÎµÎ¯ Ï„Î·Î½ Ï…ÏˆÎ·Î»ÏŒÏ„ÎµÏÎ· F1-score ÏƒÏ„Î¹Ï‚ Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎµÏ‚ Ï€ÎµÏÎ¹Ï€Ï„ÏÏƒÎµÎ¹Ï‚.
# Î¤ÎµÎ»Î¹ÎºÎ® ÎµÏ€Î¹Î»Î¿Î³Î®:
# Gradient Boosting Ï‡Ï‰ÏÎ¯Ï‚ SMOTE ÎºÎ±Î¹ Ï‡Ï‰ÏÎ¯Ï‚ Ï„Î· Î¼ÎµÎ¯Ï‰ÏƒÎ· Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½ (Î¼Îµ ÏŒÎ»Î± Ï„Î± features).

# ÎˆÏ‡ÎµÎ¹ Ï„Î·Î½ ÎºÎ±Î»ÏÏ„ÎµÏÎ· ÏƒÏ…Î½Î¿Î»Î¹ÎºÎ® Î±Ï€ÏŒÎ´Î¿ÏƒÎ·, Î¼Îµ Ï…ÏˆÎ·Î»Î® Î±ÎºÏÎ¯Î²ÎµÎ¹Î±, recall, precision ÎºÎ±Î¹ F1-score.
# Î”ÎµÎ½ Ï‡Î¬Î½ÎµÎ¹ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯Î± Î±Ï€ÏŒ Ï„Î¿ dataset.
# Î‘Î½ ÏŒÎ¼Ï‰Ï‚ Î´Î¯Î½ÎµÎ¹Ï‚ Î­Î¼Ï†Î±ÏƒÎ· ÏƒÏ„Î¿ Î½Î± Î¼ÎµÎ¹ÏÏƒÎµÎ¹Ï‚ Ï„Î· Î´Î¹Î¬ÏƒÏ„Î±ÏƒÎ· Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ ÎºÎ±Î¹ Î½Î± Î²ÎµÎ»Ï„Î¹ÏÏƒÎµÎ¹Ï‚ Ï„Î·Î½ Ï„Î±Ï‡ÏÏ„Î·Ï„Î±, Ï„ÏŒÏ„Îµ Î· ÎµÏ€Î¹Î»Î¿Î³Î® Ï„Î¿Ï… Random Forest Î¼Îµ Ï„Î± 5 ÎºÎ±Î»ÏÏ„ÎµÏÎ± Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ ÎµÎ¯Î½Î±Î¹ ÎºÎ±Î»Î® ÎµÎ½Î±Î»Î»Î±ÎºÏ„Î¹ÎºÎ®, ÎºÎ±Î¸ÏÏ‚ ÎºÏÎ±Ï„Î¬ÎµÎ¹ Î±Î¾Î¹Î¿Ï€ÏÎµÏ€Î® Î±ÎºÏÎ¯Î²ÎµÎ¹Î± Ï‡Ï‰ÏÎ¯Ï‚ Ï€ÎµÏÎ¹Ï„Ï„Î¬ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬.
#Ï€Î±Î¼Îµ Ï„Ï‰ÏÎ± Î¼Îµ Ï„Î¿ gradient boosting Î½Î± Ï€ÏÎ¿ÏƒÏ€Î±Î¸Î·ÏƒÎ¿Ï…Î¼Îµ Î½Î± Î¿ Î²ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¹Î·ÏƒÎ¿Ï…Î¼Îµ
from sklearn.model_selection import GridSearchCV
X = other_variables.copy()  # Features
y = income.values.ravel()  # Target variable

# Split dataset into 80% training and 20% testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ÎŸÏÎ¹ÏƒÎ¼ÏŒÏ‚ Ï„Î¿Ï… Gradient Boosting Classifier
gb = GradientBoostingClassifier(random_state=42)

# ÎŸÏÎ¹ÏƒÎ¼ÏŒÏ‚ Ï„Î¿Ï… grid Î³Î¹Î± hyperparameter tuning
param_grid = {
    'n_estimators': [100, 300, 500],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10],
    'subsample': [0.8, 1.0]
}

# GridSearchCV Î³Î¹Î± Ï„Î·Î½ ÎµÏÏÎµÏƒÎ· Ï„Ï‰Î½ ÎºÎ±Î»ÏÏ„ÎµÏÏ‰Î½ Ï€Î±ÏÎ±Î¼Î­Ï„ÏÏ‰Î½
grid_search = GridSearchCV(gb, param_grid, cv=3, scoring='f1', n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)

# ÎšÎ±Î»ÏÏ„ÎµÏÎµÏ‚ Ï€Î±ÏÎ¬Î¼ÎµÏ„ÏÎ¿Î¹
best_params = grid_search.best_params_
best_params
# {'learning_rate': 0.2,
#  'max_depth': 7,
#  'min_samples_split': 10,
#  'n_estimators': 500,
#  'subsample': 0.8}

#Î´Î¿ÎºÎ¹Î¼Î· Ï„Î½Ï‰ ÎºÎ±Î»Ï…ÎµÏÏ‰Î½ Ï€Î±ÏÎ±Î¼ÎµÏ„ÏÏ‰Î½ 
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
best_gb = GradientBoostingClassifier(
    learning_rate=0.2,
    max_depth=7,
    min_samples_split=10,
    n_estimators=500,
    subsample=0.8,
    random_state=42
)

# Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· Ï„Î¿Ï… Î²ÎµÎ»Ï„Î¹Ï‰Î¼Î­Î½Î¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï…
best_gb.fit(X_train, y_train)

# Î ÏÏŒÎ²Î»ÎµÏˆÎ· ÏƒÏ„Î¿ test set
y_pred_gb = best_gb.predict(X_test)

# Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ Î¼ÎµÏ„ÏÎ¹ÎºÏÎ½ Î±Ï€ÏŒÎ´Î¿ÏƒÎ·Ï‚
accuracy = accuracy_score(y_test, y_pred_gb)
precision = precision_score(y_test, y_pred_gb)
recall = recall_score(y_test, y_pred_gb)
f1 = f1_score(y_test, y_pred_gb)

# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Ï„Ï‰Î½ Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½
print(f"Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î¿ Gradient Boosting Performance:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")

# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Confusion Matrix
cm = confusion_matrix(y_test, y_pred_gb)
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=["<=50K", ">50K"], yticklabels=["<=50K", ">50K"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title(f"Confusion Matrix - Optimized Gradient Boosting")
plt.show()
#done with 4 ,no best params first gradient boosting correct




