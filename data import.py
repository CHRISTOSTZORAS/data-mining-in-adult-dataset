##import libraries and data
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from ucimlrepo import fetch_ucirepo 
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.decomposition import PCA 
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,classification_report, confusion_matrix
import time
from scipy import stats
from ucimlrepo import fetch_ucirepo
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.metrics import silhouette_score, adjusted_rand_score
from sklearn.mixture import GaussianMixture
from sklearn.cluster import DBSCAN
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression

adult = fetch_ucirepo(id=2) 
# data importing
other_variables = adult.data.features 
income = adult.data.targets 
whole_df=pd.concat([other_variables,income],axis=1)

###FIRST EXERCISE###
#data cleaning.

#we will found possible nan values
nan_counts=other_variables.isnull().sum()
nan_counts
#we find the most frecunece value in each  column
most_frequent_values = {
    "native-country": other_variables["native-country"].mode()[0],#unites-states
    "occupation": other_variables["occupation"].mode()[0],#prof-specialty
    "workclass": other_variables["workclass"].mode()[0]#private
}
print(most_frequent_values)
#now we will change the nan values with the most frequence values that we found above
other_variables["native-country"].fillna(other_variables["native-country"].mode()[0],inplace=True)
other_variables["workclass"].fillna(other_variables["workclass"].mode()[0],inplace=True)
#in occupation column we will drop nas beacuse values are even shown
other_variables.dropna(subset=["occupation"], inplace=True)

nan_counts=other_variables.isnull().sum()
nan_counts

#lets check if we have some extra noise
#LETS CHECK COLUMN AGE
other_variables['age'].unique()

#LETS CHECK COLUMN WORKCLASS
other_variables['workclass'].unique()
#we notice the question mark
other_variables.loc[:, "workclass"] = other_variables["workclass"].replace("?", most_frequent_values["workclass"])
#LETS CHECK COLUMN fnlwgt
sorted(other_variables['fnlwgt'].unique())

#LETS CHECK COLUMN education
other_variables['education'].unique()

#LETS CHECK COLUMN education-num
sorted(other_variables['education-num'].unique())

#LETS CHECK COLUMN marital-status
other_variables['marital-status'].unique()

#LETS CHECK COLUMN occupation
other_variables['occupation'].unique()
#we notice the question mark
other_variables = other_variables[other_variables["occupation"] != "?"]

#LETS CHECK COLUMN relationship
other_variables['relationship'].unique()

#LETS CHECK COLUMN race
other_variables['race'].unique()

#LETS CHECK COLUMN sex
other_variables['sex'].unique()

#LETS CHECK COLUMN capital-gain
other_variables['capital-gain'].unique()
zero_count = (other_variables['capital-gain'] == 99999).sum()
zero_count

#LETS CHECK COLUMN capital-loss
other_variables['capital-loss'].unique()
zero_count = (other_variables['capital-loss'] == 0).sum()
zero_count

#LETS CHECK COLUMN hours-per-week
other_variables['hours-per-week'].unique()

#LETS CHECK COLUMN native-country
other_variables['native-country'].unique()
#we notice the question mark
other_variables.loc[:, "native-country"] = other_variables["native-country"].replace("?", most_frequent_values["native-country"])

#LETS CHECK COLUMN income
income['income'].unique()
#we notice 2 diff types of the same value >50k &>50k. and <=50k &<=50k.
income.loc[:, 'income'] = income['income'].replace({'<=50K.': '<=50K', '>50K.': '>50K'})


#data normalization.

# We will attempt to normalize certain variables to make them more useful for the machine learning model we will use later.
# The variables being normalized are quantitative, so we focus on the numerical ones. 
# We will definitely normalize  age,weight, education number, and hours per week.
# However, we will  normalizing capital gain and capital loss with log scaling because we observed that they contain many zero values and many outliers.
# Using Min-Max Scaling in this case would create a numerical variable centered around zero with several extreme values.
columns_to_normalize = ["age", "fnlwgt", "education-num", "hours-per-week"]
scaler = MinMaxScaler()
other_variables.loc[:, columns_to_normalize] = scaler.fit_transform(other_variables[columns_to_normalize])

columns_to_log_scale = ["capital-gain", "capital-loss"]
for col in columns_to_log_scale:
    other_variables.loc[:, col] = np.log1p(other_variables[col])



#now we will also use label encoding to our nominal variables 
categorical_columns = ['workclass', 'education', 'marital-status', 'occupation', 
                       'relationship', 'race', 'sex', 'native-country']
label_encoders = {}
for col in categorical_columns:
    le = LabelEncoder()
    other_variables.loc[:, col] = le.fit_transform(other_variables[col])  # Î§ÏÎ®ÏƒÎ· .loc Î³Î¹Î± Î½Î± Î±Ï€Î¿Ï†ÏÎ³Î¿Ï…Î¼Îµ Ï„Î¿ Warning
    label_encoders[col] = le  
encoding_mappings = {col: {category: i for i, category in enumerate(label_encoders[col].classes_)} 
                     for col in categorical_columns}

encoding_df = pd.DataFrame.from_dict(encoding_mappings, orient="index").transpose()
income_encoder = LabelEncoder()
income_encoded = income_encoder.fit_transform(income.values.ravel())  # ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® ÏƒÎµ 1D array
income = pd.DataFrame(income_encoded, columns=['income'])  # ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® ÏƒÎµ DataFrame
label_encoders['income'] = income_encoder
# print(income)

#data reduction.

# Î•Î¾Î±ÏƒÏ†Î¬Î»Î¹ÏƒÎ· ÏŒÏ„Î¹ ÎºÎ±Î¹ Ï„Î± Î´ÏÎ¿ datasets Î­Ï‡Î¿Ï…Î½ Î¯Î´Î¹Î¿ index
other_variables = other_variables.reset_index(drop=True)
income = income.reset_index(drop=True)
common_index = other_variables.index.intersection(income.index)
# Î”Î¹Î±Ï„Î®ÏÎ·ÏƒÎ· Î¼ÏŒÎ½Î¿ Ï„Ï‰Î½ ÎºÎ¿Î¹Î½ÏÎ½ Î³ÏÎ±Î¼Î¼ÏÎ½
other_variables = other_variables.loc[common_index].reset_index(drop=True)
income = income.loc[common_index].reset_index(drop=True)
# Apply PCA while keeping 95% of the variance
pca = PCA(n_components=0.95)  # Automatically selects the number of components
pct = pca.fit_transform(other_variables)
num_components = pca.n_components_
pca_columns = [f'pc{i+1}' for i in range(num_components)]
principal_df = pd.DataFrame(pct, columns=pca_columns)
finaldf = pd.concat([principal_df, income], axis=1)
# Print the transformed dataset
print(f"Number of selected principal components: {num_components}")
print(finaldf)



###SECOND EXERCISE###

#Future Selection.

#Feature Selection using SelectKBest
features = other_variables
target = income.values.ravel()
selector = SelectKBest(score_func=f_classif, k=5) 
features_new = selector.fit_transform(features, target)
selected_features_kbest = features.columns[selector.get_support()]
selected_features_df = pd.DataFrame({
    "Method": ["SelectKBest"] * 5 ,
    "Feature": selected_features_kbest.tolist() 
})
print(selected_features_df)

#Training Models Full and with 5 Features

features_full = other_variables
features_reduced = other_variables[selected_features_kbest]
target= income
X_train_full, X_test_full, y_train, y_test = train_test_split(features_full, target, test_size=0.2, random_state=42)
X_train_reduced, X_test_reduced, y_train_reduced, y_test_reduced = train_test_split(features_reduced, target, test_size=0.2, random_state=42)

# Training Decision Tree  Full
start_time = time.time()
dt_full = DecisionTreeClassifier(random_state=42)
dt_full.fit(X_train_full, y_train)
y_pred_full = dt_full.predict(X_test_full)
accuracy_full = accuracy_score(y_test, y_pred_full)
time_full = time.time() - start_time

# Decision Tree 5 Variables
start_time = time.time()
dt_reduced = DecisionTreeClassifier(random_state=42)
dt_reduced.fit(X_train_reduced, y_train_reduced)
y_pred_reduced = dt_reduced.predict(X_test_reduced)
accuracy_reduced = accuracy_score(y_test_reduced, y_pred_reduced)
time_reduced = time.time() - start_time

# Print Results
comparison_df = pd.DataFrame({
    "Model": ["Full Feature Set", "Reduced Feature Set (Top 5)"],
    "Accuracy": [accuracy_full, accuracy_reduced],
    "Training Time (seconds)": [time_full, time_reduced]
})
print(comparison_df)
#we notice that dont only with 5 variables we get a better accuracy index but its also a lot more quicker


#Managing Outliers.

#detect outliers with iqr
selected_columns = ["age", "capital-gain", "capital-loss", "hours-per-week"]
z_scores = np.abs(stats.zscore(whole_df[selected_columns]))
threshold = 3.5
outliers = (z_scores > threshold).sum(axis=0)
outliers_summary_zscore = pd.DataFrame({
    "Column": selected_columns,
    "Number of Outliers": outliers
})
print(outliers_summary_zscore)

#plot out outliers
def plot_outliers_boxplots(data, selected_columns):
    for column in selected_columns:
        plt.figure(figsize=(8, 5))
        sns.boxplot(x=data[column])
        plt.title(f"Boxplot of {column} (Detecting Outliers)")
        plt.show()
plot_outliers_boxplots(other_variables, selected_columns)

#we plot out corellation amtrix in order to see the corellation between our columns in relation with our our target value
correlation_matrix = other_variables.corr()
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm", linewidths=0.5)
plt.title("Correlation Matrix of Features")
plt.show()
#manage outliers
#fo fnlwgt column we see that correlation is not much so we can throw it out from our data, it doesnt help predict income.
#for age and hours per week we will cap extreme values at the 99th precentile 
# 1ï¸âƒ£ Î—Î»Î¹ÎºÎ¯Î± (age) - Î“Î¹Î±Ï„Î¯ ÎºÏŒÎ²Î¿Ï…Î¼Îµ Î¼ÏŒÎ½Î¿ Ï„Î¿ upper bound;
# ÎŸÎ¹ Ï„Î¹Î¼Î­Ï‚ Ï„Î·Ï‚ Î·Î»Î¹ÎºÎ¯Î±Ï‚ ÎµÎ¯Î½Î±Î¹ Ï€Î¬Î½Ï„Î± Î¸ÎµÏ„Î¹ÎºÎ­Ï‚ ÎºÎ±Î¹ Î­Ï‡Î¿Ï…Î½ Ï†Ï…ÏƒÎ¹ÎºÏŒ ÎºÎ±Ï„ÏÏ„Î±Ï„Î¿ ÏŒÏÎ¹Î¿ (Ï€.Ï‡. ÎºÎ±Î½ÎµÎ¯Ï‚ Î´ÎµÎ½ ÎµÎ¯Î½Î±Î¹ ÎºÎ¬Ï„Ï‰ Î±Ï€ÏŒ 18).
# Î— ÎºÎ±Ï„Î±Î½Î¿Î¼Î® Ï„Î·Ï‚ Î·Î»Î¹ÎºÎ¯Î±Ï‚ ÎµÎ¯Î½Î±Î¹ Î´ÎµÎ¾Î¹Î¬ Î±ÏƒÏÎ¼Î¼ÎµÏ„ÏÎ· (right-skewed), ÏŒÏ€Î¿Ï… Î»Î¯Î³ÎµÏ‚ Ï„Î¹Î¼Î­Ï‚ (Ï€.Ï‡. 90-100 ÎµÏ„ÏÎ½) ÎµÎ¼Ï†Î±Î½Î¯Î¶Î¿Î½Ï„Î±Î¹ ÏƒÏ€Î¬Î½Î¹Î± ÎºÎ±Î¹ Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¿ÏÎ½ Ï‰Ï‚ outliers.
# Î— Î·Î»Î¹ÎºÎ¯Î± Î´ÎµÎ½ Î­Ï‡ÎµÎ¹ Î±ÎºÏÎ±Î¯Î± Î¼Î¹ÎºÏÎ­Ï‚ Ï„Î¹Î¼Î­Ï‚ Ï€Î¿Ï… Î½Î± Ï‡ÏÎµÎ¹Î¬Î¶Î¿Î½Ï„Î±Î¹ Ï‡Î±Î¼Î®Î»Ï‰Î¼Î±.
# ğŸ“Œ Î›ÏÏƒÎ·:
# âœ… Î•Ï†Î±ÏÎ¼ÏŒÎ¶Î¿Ï…Î¼Îµ upper bound ÏƒÏ„Î¿ 99Î¿ percentile Î³Î¹Î± Î½Î± Ï€ÎµÏÎ¹Î¿ÏÎ¯ÏƒÎ¿Ï…Î¼Îµ Î¼ÏŒÎ½Î¿ Ï„Î¹Ï‚ Ï…Ï€ÎµÏÎ²Î¿Î»Î¹ÎºÎ¬ Î¼ÎµÎ³Î¬Î»ÎµÏ‚ Ï„Î¹Î¼Î­Ï‚, Ï‡Ï‰ÏÎ¯Ï‚ Î½Î± Ï€ÎµÎ¹ÏÎ¬Î¾Î¿Ï…Î¼Îµ Ï„Î¹Ï‚ Ï‡Î±Î¼Î·Î»Î­Ï‚ Ï„Î¹Î¼Î­Ï‚.
# 2ï¸âƒ£ ÎÏÎµÏ‚ ÎµÏÎ³Î±ÏƒÎ¯Î±Ï‚ Î±Î½Î¬ ÎµÎ²Î´Î¿Î¼Î¬Î´Î± (hours-per-week) - Î“Î¹Î±Ï„Î¯ ÎºÏŒÎ²Î¿Ï…Î¼Îµ Î¼ÏŒÎ½Î¿ Ï„Î¿ upper bound;
# Î— ÏƒÏ„Î®Î»Î· "ÏÏÎµÏ‚ ÎµÏÎ³Î±ÏƒÎ¯Î±Ï‚ Î±Î½Î¬ ÎµÎ²Î´Î¿Î¼Î¬Î´Î±" Î­Ï‡ÎµÎ¹ Ï†Ï…ÏƒÎ¹ÎºÎ¬ ÏŒÏÎ¹Î±: 0-168 (Î±Î½ ÎºÎ¬Ï€Î¿Î¹Î¿Ï‚ ÎµÏÎ³Î±Î¶ÏŒÏ„Î±Î½ 24/7).
# ÎŸÎ¹ Î¼Î¹ÎºÏÎ­Ï‚ Ï„Î¹Î¼Î­Ï‚ (0-10 ÏÏÎµÏ‚) ÎµÎ¯Î½Î±Î¹ Î»Î¿Î³Î¹ÎºÎ­Ï‚ ÎºÎ±Î¹ Î´ÎµÎ½ Î±Ï€Î¿Ï„ÎµÎ»Î¿ÏÎ½ Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÎ¬ outliers.
# ÎŸÎ¹ Ï…Ï€ÎµÏÎ²Î¿Î»Î¹ÎºÎ¬ Î¼ÎµÎ³Î¬Î»ÎµÏ‚ Ï„Î¹Î¼Î­Ï‚ (Ï€.Ï‡. >80 ÏÏÎµÏ‚ Ï„Î·Î½ ÎµÎ²Î´Î¿Î¼Î¬Î´Î±) Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± ÎµÎ¯Î½Î±Î¹ Î±ÎºÏÎ±Î¯ÎµÏ‚ Ï€ÎµÏÎ¹Ï€Ï„ÏÏƒÎµÎ¹Ï‚ Î® Î»Î¬Î¸Î· ÏƒÏ„Î·Î½ ÎºÎ±Ï„Î±Î³ÏÎ±Ï†Î®.
# ğŸ“Œ Î›ÏÏƒÎ·:
# âœ… Î•Ï†Î±ÏÎ¼ÏŒÎ¶Î¿Ï…Î¼Îµ Î¼ÏŒÎ½Î¿ Î­Î½Î± upper bound (Ï€.Ï‡. 99Î¿ percentile) Î³Î¹Î± Î½Î± Î±Ï†Î±Î¹ÏÎ­ÏƒÎ¿Ï…Î¼Îµ Ï„Î¹Î¼Î­Ï‚ Ï€Î¿Ï… ÎµÎ¯Î½Î±Î¹ Î±Ï†ÏÏƒÎ¹ÎºÎ± Î¼ÎµÎ³Î¬Î»ÎµÏ‚ (>80-90 ÏÏÎµÏ‚), Ï‡Ï‰ÏÎ¯Ï‚ Î½Î± Ï€ÎµÎ¹ÏÎ¬Î¾Î¿Ï…Î¼Îµ Ï‡Î±Î¼Î·Î»Î­Ï‚ Ï„Î¹Î¼Î­Ï‚ Ï€Î¿Ï… Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Î­Ï‡Î¿Ï…Î½ Î½ÏŒÎ·Î¼Î±.

# for capital gain and capital loss ,we already have logarithm used we will cap them too
#Î¸Î± Î±Ï„Î½Î¹Î±ÎºÎ±Ï„ÏƒÎ¿Ï…Î¼Îµ Î´Î·Î»Î±Î´Î· Ï„Î¹Ï‚ Ï€Î¿Î»Ï… Î¼Î¹ÏÎµÎºÏ‚ Ï„Î¹Î¼ÎµÏ‚ Î¼Îµ Ï„Î·Î½ Ï„Î¹Î¼Î· Ï„Î¿Ï… 5Î¿Ï… ÎµÎºÎ±Ï„Î¿ÏƒÏ„Î·Î¼Î¿ÏÎ¹Î¿Ï… 
#ÎºÎ±Î¹ Ï„Î¹Ï‚ Ï„Î¹Ï‚ Ï€Î¿Î»Ï… Î¼ÎµÎ³Î±Î»ÎµÏ‚ Ï„Î¹Î¼ÎµÏ‚ Î¼Îµ Ï„Î·Î½ Ï„Î¹Î¼Î· Ï„Î¿Ï… 95Î¿Ï… ÎµÎºÎ±Ï„Î·ÏƒÏ„Î¿Î¼Î¿ÏÎ¹Î¿Ï… 
#Î´ÎµÎ½ Î¼Ï€Î¿ÏÎ¿Ï…ÏƒÎ±Î¼Îµ Î½Î± Ï„Î¹Ï‚ Î´Î¹Ï‰Î¾Î¿Ï…Î¼Îµ Î³Î¹Î±Ï„Î¹ ÎµÎ¹Î´ÏƒÎ±Î¼Îµ Î±Ï€Î¿ Ï„Î¿ Î´Î¹Î±Î³ÏÎ±Î¼Î¼Î± Î¿Ï„Î¹ Î¹ÏƒÏ‰Ï‚ Ï€Î±ÏÎµÏ‡Î¿Ï…Î½ Î¼Î¹Î± ÏƒÎ·Î¼Î±Î½Ï„Î¹ÎºÎ· Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¹Î±


other_variables = other_variables.drop(columns=["fnlwgt"])
for col in ["age", "hours-per-week"]:
    upper_limit = other_variables[col].quantile(0.99)  # Get 99th percentile value
    other_variables[col] = np.where(other_variables[col] > upper_limit, upper_limit, other_variables[col]) 
    
for col in ["capital-gain", "capital-loss"]:
    lower_limit = other_variables[col].quantile(0.05)  # 1Î¿ percentile
    upper_limit = other_variables[col].quantile(0.95)  # 99Î¿ percentile
    other_variables[col] = np.clip(other_variables[col], lower_limit, upper_limit) 
    
#Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ Î±Î½ Î¾Î±Î½Î± Î´Î¿Ï…Î¼Îµ Ï„Î¹Ï‚ ÎºÎ±ÏÎ±Î¹ÎµÏ‚ Ï„Î¹Î¼ÎµÏ‚ Î¼Î±Ï‚ Î¸Î± Î´Î¿Ï…Î¼Îµ Î¿Ï„Î¹ Î¼Î·Î´ÎµÎ½Î¹ÏƒÏ„Î·ÎºÎ±Î½
#detect outliers with iqr
selected_columns = ["age", "capital-gain", "capital-loss", "hours-per-week"]
z_scores = np.abs(stats.zscore(other_variables[selected_columns]))
threshold = 3.5
outliers = (z_scores > threshold).sum(axis=0)
outliers_summary_zscore = pd.DataFrame({
    "Column": selected_columns,
    "Number of Outliers": outliers
})
print(outliers_summary_zscore)

###THIRD EXERCISE###
# ğŸš€ ÎšÎ¬Î½Î¿Ï…Î¼Îµ Clustering ÏƒÏ„Î¿ principal_df Î³Î¹Î±Ï„Î¯ Ï„Î¿ PCA Î²Î¿Î·Î¸Î¬ ÏƒÏ„Î·Î½ ÎºÎ±Î»ÏÏ„ÎµÏÎ· Î±Î½Î¬Î»Ï…ÏƒÎ· Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½, Î±Ï€Î¿Ï†ÎµÏÎ³ÎµÎ¹ Ï„Î¿Î½ Î¸ÏŒÏÏ…Î²Î¿ ÎºÎ±Î¹ Î²ÎµÎ»Ï„Î¹ÏÎ½ÎµÎ¹ Ï„Î·Î½ Ï„Î±Ï‡ÏÏ„Î·Ï„Î±.
# ğŸ“Œ Î‘Î½ Î­Ï‡ÎµÎ¹Ï‚ Î»Î¯Î³ÎµÏ‚ Î´Î¹Î±ÏƒÏ„Î¬ÏƒÎµÎ¹Ï‚ (<10 features), Î¼Ï€Î¿ÏÎµÎ¯Ï‚ Î½Î± Î´Î¿ÎºÎ¹Î¼Î¬ÏƒÎµÎ¹Ï‚ ÎºÎ±Î¹ clustering ÏƒÏ„Î¿ Î±ÏÏ‡Î¹ÎºÏŒ dataset ÎºÎ±Î¹ Î½Î± ÏƒÏ…Î³ÎºÏÎ¯Î½ÎµÎ¹Ï‚.
X_clustering = principal_df.copy()  # Using PCA-transformed features

# Apply K-Means Clustering with 2 clusters (matching income categories <=50K and >50K)
kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
kmeans_labels = kmeans.fit_predict(X_clustering)

# Apply Agglomerative Clustering
agglo = AgglomerativeClustering(n_clusters=2)
agglo_labels = agglo.fit_predict(X_clustering)

# Evaluate Clustering Performance
silhouette_kmeans = silhouette_score(X_clustering, kmeans_labels)
silhouette_agglo = silhouette_score(X_clustering, agglo_labels)
ari_kmeans = adjusted_rand_score(income.values.ravel(), kmeans_labels)
ari_agglo = adjusted_rand_score(income.values.ravel(), agglo_labels)

# Create DataFrame to store clustering results
clustering_results = pd.DataFrame({
    "Algorithm": ["K-Means", "Agglomerative"],
    "Silhouette Score": [silhouette_kmeans, silhouette_agglo],
    "Adjusted Rand Index": [ari_kmeans, ari_agglo]
})
print(clustering_results)

# Plot Clustering Results using PCA projection
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# K-Means plot
axes[0].scatter(principal_df.iloc[:, 0], principal_df.iloc[:, 1], c=kmeans_labels, cmap='coolwarm', alpha=0.5)
axes[0].set_title("K-Means Clustering (PCA Projection)")

# Agglomerative plot
axes[1].scatter(principal_df.iloc[:, 0], principal_df.iloc[:, 1], c=agglo_labels, cmap='coolwarm', alpha=0.5)
axes[1].set_title("Agglomerative Clustering (PCA Projection)")

plt.show()
# 1ï¸âƒ£ Î¤Î¿ Clustering Î´Î·Î¼Î¹Î¿ÏÏÎ³Î·ÏƒÎµ ÎºÎ±Î¸Î±ÏÎ­Ï‚ Î¿Î¼Î¬Î´ÎµÏ‚, Î±Î»Î»Î¬ Î±Ï…Ï„Î­Ï‚ Î´ÎµÎ½ Î±Î½Ï„Î¹ÎºÎ±Ï„Î¿Ï€Ï„ÏÎ¯Î¶Î¿Ï…Î½ Ï„Î¿ Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÏŒ ÎµÎ¹ÏƒÏŒÎ´Î·Î¼Î±.
# 2ï¸âƒ£ Î¤Î± Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Î´ÎµÎ½ ÎµÎ¯Î½Î±Î¹ Î±ÏÎºÎµÏ„Î¬ Î´Î¹Î±ÎºÏÎ¹Ï„Î¬ ÏÏƒÏ„Îµ Î½Î± Î´Î¹Î±Ï‡Ï‰ÏÎ¯ÏƒÎ¿Ï…Î½ Ï„Î¿Ï…Ï‚ Î±Î½Î¸ÏÏÏ€Î¿Ï…Ï‚ ÏƒÎµ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯ÎµÏ‚ ÎµÎ¹ÏƒÎ¿Î´Î®Î¼Î±Ï„Î¿Ï‚.
# 3ï¸âƒ£ Î¤Î¿ PCA Î²Î¿Î®Î¸Î·ÏƒÎµ ÏƒÏ„Î¿Î½ Î´Î¹Î±Ï‡Ï‰ÏÎ¹ÏƒÎ¼ÏŒ Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½, Î±Î»Î»Î¬ Î´ÎµÎ½ ÎµÏ€Î±ÏÎºÎµÎ¯ Î³Î¹Î± ÏƒÏ‰ÏƒÏ„Î® Ï€ÏÏŒÎ²Î»ÎµÏˆÎ· ÎµÎ¹ÏƒÎ¿Î´Î®Î¼Î±Ï„Î¿Ï‚.
# 4ï¸âƒ£ ÎŠÏƒÏ‰Ï‚ Î±Ï€Î±Î¹Ï„Î¿ÏÎ½Ï„Î±Î¹ Ï€Î¹Î¿ Î¹ÏƒÏ‡Ï…ÏÎ¬ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ (Ï€.Ï‡. ÎµÏÎ³Î±ÏƒÎ¹Î±ÎºÎ® ÎµÎ¼Ï€ÎµÎ¹ÏÎ¯Î±, ÎµÏ€Î¯Ï€ÎµÎ´Î¿ ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·Ï‚ Î¼Îµ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÏŒ Ï„ÏÏŒÏ€Î¿), Î® Î¬Î»Î»ÎµÏ‚ Î¼Î­Î¸Î¿Î´Î¿Î¹ ÏŒÏ€Ï‰Ï‚ supervised learning (Ï€.Ï‡. classification).
# Î”Î¿ÎºÎ¹Î¼Î® Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÏÎ½ Î±ÏÎ¹Î¸Î¼ÏÎ½ clusters

#Ï€Î±Î¼Îµ ÎµÏ€Î¹ÏƒÎ·Ï‚ Î½Î± Î´ÎºÎ¹Î¼Î±ÏƒÎ¿Ï…Î¼Îµ Î¼ÎµÏÎ¹ÎºÎ¿Ï…Ï‚ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ¿Ï…Ï‚ Ï„ÏÏŒÏ€Î¿Ï…Ï‚ Î¿Ï€Ï‰Ï‚ gauss Î· dbscan ÎºÎ±Î¹ Î³Î¹Î± Ï€ÎµÏÎ¹ÏƒÏƒÎ¿Ï„ÎµÏÎµÏ‚ ÏƒÏ…ÏƒÏ„Î±Î´ÎµÏ‚ Î½Î± Î´Î¿Ï…Î¼Îµ 
#Î±Î½ Î¼Ï€Î¿ÏÎ¿Ï…Î¼Îµ ÎºÎ±Ï€Ï‰Ï‚ Î½Î± Î²Î»ÎµÏ„Î¹ÏÏƒÎ¿Ï…Î¼Îµ Ï„Î·Î½ ÏƒÏ…ÏƒÏ„Î±Î´Î¿Ï€Î¿Î¹Î·ÏƒÎ± Î¼Î±Ï‚

################# ÎœÎ—Î Î¤ÎŸ ÎÎ‘ÎÎ‘Î¤Î¡Î•ÎÎ•Î™Î£ ÎŸÎ¤Î‘Î Î Î•Î¡Î‘Î£ Î¤Î—Î Î•Î¡Î“Î‘Î£Î™Î‘ Î¥Î Î‘Î¡Î§Î•Î™ Î¤ÎŸ DF SCREENSHOT Î£Î¤ÎŸ Î¦Î‘ÎšÎ•Î›ÎŸ Î¤Î—Ï‚ Î•Î¡Î“Î‘Î£Î™Î‘Ï‚ #################
cluster_numbers = [2, 3, 4, 5]
X_clustering = principal_df.copy()
# Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½
clustering_results = []
for k in cluster_numbers:
        # K-Means Clustering
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans_labels = kmeans.fit_predict(X_clustering)
        silhouette_kmeans = silhouette_score(X_clustering, kmeans_labels)

        # Agglomerative Clustering
        agglo = AgglomerativeClustering(n_clusters=k)
        agglo_labels = agglo.fit_predict(X_clustering)
        silhouette_agglo = silhouette_score(X_clustering, agglo_labels)

        # DBSCAN Clustering
        dbscan = DBSCAN(eps=1.5, min_samples=5)  # Î˜Î± Ï‡ÏÎµÎ¹Î±ÏƒÏ„ÎµÎ¯ tuning Ï„Î¿Ï… eps
        dbscan_labels = dbscan.fit_predict(X_clustering)
        silhouette_dbscan = silhouette_score(X_clustering, dbscan_labels) if len(set(dbscan_labels)) > 1 else None

        # Gaussian Mixture Model (GMM)
        gmm = GaussianMixture(n_components=k, random_state=42)
        gmm_labels = gmm.fit_predict(X_clustering)
        silhouette_gmm = silhouette_score(X_clustering, gmm_labels)

        # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½
        clustering_results.append({
            "Clusters": k,
            "Algorithm": "K-Means",
            "Silhouette Score": silhouette_kmeans
        })
        clustering_results.append({
            "Clusters": k,
            "Algorithm": "Agglomerative",
            "Silhouette Score": silhouette_agglo
        })
        if silhouette_dbscan:
            clustering_results.append({
                "Clusters": k,
                "Algorithm": "DBSCAN",
                "Silhouette Score": silhouette_dbscan
            })
        clustering_results.append({
            "Clusters": k,
            "Algorithm": "GMM",
            "Silhouette Score": silhouette_gmm
        })

# ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® ÏƒÎµ DataFrame ÎºÎ±Î¹ ÎµÎ¼Ï†Î¬Î½Î¹ÏƒÎ·
results_df = pd.DataFrame(clustering_results)
print(results_df)
# ğŸ“Œ 2ï¸âƒ£ Î£Ï…Î¼Ï€ÎµÏÎ¬ÏƒÎ¼Î±Ï„Î± Î³Î¹Î± Ï„Î¿Î½ Î‘ÏÎ¹Î¸Î¼ÏŒ Ï„Ï‰Î½ Clusters
# Î¤Î¿ K=2 ÎµÎ¯Î½Î±Î¹ Î· ÎºÎ±Î»ÏÏ„ÎµÏÎ· ÎµÏ€Î¹Î»Î¿Î³Î®, ÎºÎ±Î¸ÏÏ‚ Î¿Î´Î·Î³ÎµÎ¯ ÏƒÎµ Ï…ÏˆÎ·Î»ÏŒÏ„ÎµÏÎ· ÏƒÏ…Î½Î¿Ï‡Î® Ï„Ï‰Î½ ÏƒÏ…ÏƒÏ„Î¬Î´Ï‰Î½.
# Î‘ÏÎ¾Î·ÏƒÎ· Ï„Î¿Ï… K ÏƒÎµ 3,4,5 Î¼ÎµÎ¹ÏÎ½ÎµÎ¹ Ï„Î¿ Silhouette Score, Ï€ÏÎ¬Î³Î¼Î± Ï€Î¿Ï… ÏƒÎ·Î¼Î±Î¯Î½ÎµÎ¹ ÏŒÏ„Î¹ Î¿Î¹ Î¿Î¼Î¬Î´ÎµÏ‚ Î´ÎµÎ½ ÎµÎ¯Î½Î±Î¹ Ï„ÏŒÏƒÎ¿ ÎºÎ±Î»Î¬ Î´Î¹Î±Ï‡Ï‰ÏÎ¹ÏƒÎ¼Î­Î½ÎµÏ‚.
# Î¤Î¿ Î±Ï€Î¿Ï„Î­Î»ÎµÏƒÎ¼Î± ÎµÏ€Î¹Î²ÎµÎ²Î±Î¹ÏÎ½ÎµÎ¹ ÏŒÏ„Î¹ Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î¼Î±Ï‚ Ï„ÎµÎ¯Î½Î¿Ï…Î½ Î½Î± Î´Î¹Î±Ï‡Ï‰ÏÎ¯Î¶Î¿Î½Ï„Î±Î¹ ÏƒÎµ 2 ÎºÏÏÎ¹ÎµÏ‚ Î¿Î¼Î¬Î´ÎµÏ‚, Ï€Î¿Ï… Ï€Î¹Î¸Î±Î½ÏŒÏ„Î±Ï„Î± Î±Î½Ï„Î¹ÏƒÏ„Î¿Î¹Ï‡Î¿ÏÎ½ ÏƒÏ„Î¿ ÎµÎ¹ÏƒÏŒÎ´Î·Î¼Î± (<=50K, >50K).
# ğŸ“Œ 3ï¸âƒ£ Î“Î¹Î±Ï„Î¯ Ï„Î¿ DBSCAN Î‘Ï€Î­Ï„Ï…Ï‡Îµ;
# Î¤Î¿ DBSCAN Î´ÎµÎ½ Î´Î¿Ï…Î»ÎµÏÎµÎ¹ ÎºÎ±Î»Î¬ ÏƒÏ„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î¼Î±Ï‚ Î»ÏŒÎ³Ï‰ Ï„Î¿Ï… Ï„ÏÏŒÏ€Î¿Ï… Î¼Îµ Ï„Î¿Î½ Î¿Ï€Î¿Î¯Î¿ Î±Î½Î±Î³Î½Ï‰ÏÎ¯Î¶ÎµÎ¹ Ï„Î¹Ï‚ ÏƒÏ…ÏƒÏ„Î¬Î´ÎµÏ‚.
# Î•Î¯Î½Î±Î¹ Ï€Î¹Î¿ Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î±Ï„Î¹ÎºÏŒ ÏƒÎµ Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î¼Îµ ÏƒÎ±Ï†ÏÏ‚ Î´Î¹Î±Ï‡Ï‰ÏÎ¹ÏƒÎ¼Î­Î½Î± clusters, ÎºÎ¬Ï„Î¹ Ï€Î¿Ï… Î´ÎµÎ½ Ï†Î±Î¯Î½ÎµÏ„Î±Î¹ Î½Î± Î¹ÏƒÏ‡ÏÎµÎ¹ ÎµÎ´Ï.
# Î¤Î± Î±ÏÎ½Î·Ï„Î¹ÎºÎ¬ Silhouette Scores ÏƒÎ·Î¼Î±Î¯Î½Î¿Ï…Î½ ÏŒÏ„Î¹ Î¿ DBSCAN Î´Î·Î¼Î¹Î¿ÏÏÎ³Î·ÏƒÎµ Ï€Î¿Î»Ï Î±Î´ÏÎ½Î±Î¼ÎµÏ‚ Î® Ï…Ï€ÎµÏÎ²Î¿Î»Î¹ÎºÎ¬ Î´Î¹Î¬ÏƒÏ€Î±ÏÏ„ÎµÏ‚ ÏƒÏ…ÏƒÏ„Î¬Î´ÎµÏ‚.
# ğŸ“Œ 4ï¸âƒ£ Î£Ï…Î¼Ï€Î­ÏÎ±ÏƒÎ¼Î± ÎºÎ±Î¹ Î’Î­Î»Ï„Î¹ÏƒÏ„Î· Î¡ÏÎ¸Î¼Î¹ÏƒÎ·
# ğŸ”¹ ÎŸ ÎºÎ±Î»ÏÏ„ÎµÏÎ¿Ï‚ Î±Î»Î³ÏŒÏÎ¹Î¸Î¼Î¿Ï‚: Agglomerative Clustering (K=2) Î¼Îµ Silhouette Score = 0.72.
# ğŸ”¹ ÎŸ K-Means Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³ÎµÎ¯ ÎºÎ±Î»ÏÏ„ÎµÏÎ± ÏŒÏ„Î±Î½ Î±Ï…Î¾Î¬Î½Î¿Ï…Î¼Îµ Ï„Î± clusters, Î±Î»Î»Î¬ Î· ÏƒÏ…Î½Î¿Ï‡Î® Î¼ÎµÎ¹ÏÎ½ÎµÏ„Î±Î¹.
# ğŸ”¹ ÎŸ DBSCAN Î´ÎµÎ½ ÎµÎ¯Î½Î±Î¹ ÎºÎ±Ï„Î¬Î»Î»Î·Î»Î¿Ï‚ Î³Î¹Î± Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î¼Î±Ï‚.
# ğŸ”¹ ÎŸ GMM Î´ÎµÎ½ Î±Ï€Î¿Î´Î¯Î´ÎµÎ¹ ÎºÎ±Î»Î¬, Ï€Î¹Î¸Î±Î½ÏÏ‚ ÎµÏ€ÎµÎ¹Î´Î® Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î¼Î±Ï‚ Î´ÎµÎ½ Î­Ï‡Î¿Ï…Î½ ÏƒÎ±Ï†Î® Gaussian ÎºÎ±Ï„Î±Î½Î¿Î¼Î®.

# ğŸš€ Î¤ÎµÎ»Î¹ÎºÎ® Î‘Ï€ÏŒÏ†Î±ÏƒÎ·: Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Agglomerative Clustering Î¼Îµ K=2 Î³Î¹Î± Ï„Î·Î½ ÎºÎ±Î»ÏÏ„ÎµÏÎ· Î¿Î¼Î±Î´Î¿Ï€Î¿Î¯Î·ÏƒÎ·.
#Î•Î Î™Î£Î—Î£ Î”Î•Î ÎœÎ ÎŸÎ¡ÎŸÎ¥ÎœÎ• ÎÎ‘ ÎšÎ‘ÎÎ¥ÎŸÎœÎ• ÎšÎ‘Î¤Î™ Î“Î™Î‘ ÎÎ‘ Î’Î•Î›Î¤Î™Î£Î¤ÎŸÎ ÎŸÎ™Î˜Î—Î£ÎŸÎ¥ÎœÎ• Î¤ÎŸÎ¥Î£ Î‘Î›Î“ÎŸÎ¡Î™Î˜ÎœÎŸÎ¥Î£ ÎœÎ‘Î£ ÎŸÎ ÎŸÎ¤Î• ÎœÎ•ÎÎŸÎ¥ÎœÎ• ÎœÎ• Î¤Î™Ï‚ Î‘Î¡Î§Î™ÎšÎ•Î£ Î˜Î•Î©Î¡Î™Î•Î£

###FORTH EXERCISE###
X = other_variables.copy()  # Features
y = income.values.ravel() # Target variable
pd.Series(y).value_counts()
# Split dataset into 80% training and 20% testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
models = {
    "Logistic Regression": LogisticRegression(max_iter=500, class_weight='balanced'),
    "Random Forest": RandomForestClassifier(n_estimators=300, random_state=42, class_weight='balanced'),
    "Support Vector Machine": SVC(class_weight='balanced'),
    "Gradient Boosting": GradientBoostingClassifier(n_estimators=100, random_state=42)
}

results = []

for model_name, model in models.items():
    # Train model
    model.fit(X_train, y_train)
    
    # Predict on test set
    y_pred = model.predict(X_test)
    
    # Evaluate results
    accuracy = accuracy_score(y_test, y_pred)
    precision = classification_report(y_test, y_pred, output_dict=True)["weighted avg"]["precision"]
    recall = classification_report(y_test, y_pred, output_dict=True)["weighted avg"]["recall"]
    f1 = classification_report(y_test, y_pred, output_dict=True)["weighted avg"]["f1-score"]
    
    # Store results
    results.append({"Model": model_name, "Accuracy": accuracy, "Precision": precision, "Recall": recall, "F1-Score": f1})
    
    # Plot confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(5,4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=["<=50K", ">50K"], yticklabels=["<=50K", ">50K"])
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title(f"Confusion Matrix - {model_name}")
    plt.show()

# 3ï¸âƒ£ Display Results
machine_learning_results_df = pd.DataFrame(results)
print(machine_learning_results_df)

# ÎŸÎ¹ confusion matrices Î´ÎµÎ¯Ï‡Î½Î¿Ï…Î½ Ï„Î·Î½ Î±Ï€ÏŒÎ´Î¿ÏƒÎ· ÎºÎ¬Î¸Îµ Î¼Î¿Î½Ï„Î­Î»Î¿Ï… ÏƒÎµ Î´ÏÎ¿ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯ÎµÏ‚:

# <=50K (Î§Î±Î¼Î·Î»ÏŒ Î•Î¹ÏƒÏŒÎ´Î·Î¼Î±)
# >50K (Î¥ÏˆÎ·Î»ÏŒ Î•Î¹ÏƒÏŒÎ´Î·Î¼Î±)
# ğŸ”¹ Logistic Regression
# True Positives (TP) = 2934
# False Negatives (FN) = 0 (Î¬ÏÎ± Î´ÎµÎ½ Ï€ÏÎ¿Î²Î»Î­Ï€ÎµÎ¹ ÏƒÏ‡ÎµÎ´ÏŒÎ½ ÎºÎ±Î¸ÏŒÎ»Î¿Ï… ÏƒÏ‰ÏƒÏ„Î¬ Ï„Î¿ >50K)
# False Positives (FP) = 2934 (Ï€Î¿Î»Î»Î¬ Î»Î¬Î¸Î¿Ï‚ predictions Î³Î¹Î± <=50K)
# Accuracy = 54.2% (Ï‡Î±Î¼Î·Î»ÏŒ)
# ğŸ“Œ Î£Ï…Î¼Ï€Î­ÏÎ±ÏƒÎ¼Î±: Î¤Î¿ Logistic Regression Î´ÎµÎ½ Î±Ï€Î¿Î´Î¯Î´ÎµÎ¹ ÎºÎ±Î»Î¬, ÎºÎ±Î¸ÏÏ‚ Î¼Ï€ÎµÏÎ´ÎµÏÎµÎ¹ Î±ÏÎºÎµÏ„Î¬ Ï„Î¹Ï‚ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯ÎµÏ‚ ÎºÎ±Î¹ Î­Ï‡ÎµÎ¹ Ï‡Î±Î¼Î·Î»ÏŒ Accuracy.

# ğŸ”¹ Random Forest
# True Positives (TP) = 809 (Î»Î¯Î³ÎµÏ‚ ÏƒÏ‰ÏƒÏ„Î­Ï‚ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ >50K)
# False Negatives (FN) = 0 (Î±ÎºÏŒÎ¼Î± ÏƒÎ·Î¼Î±Î½Ï„Î¹ÎºÏŒ Ï€ÏÏŒÎ²Î»Î·Î¼Î±)
# Accuracy = 69.7% (Î¼Î­Ï„ÏÎ¹Î¿)
# ğŸ“Œ Î£Ï…Î¼Ï€Î­ÏÎ±ÏƒÎ¼Î±: Î¤Î¿ Random Forest ÎµÎ¯Î½Î±Î¹ ÏƒÎ·Î¼Î±Î½Ï„Î¹ÎºÎ¬ ÎºÎ±Î»ÏÏ„ÎµÏÎ¿ Î±Ï€ÏŒ Ï„Î¿ Logistic Regression, Î±Î»Î»Î¬ Î´Ï…ÏƒÎºÎ¿Î»ÎµÏÎµÏ„Î±Î¹ Î±ÎºÏŒÎ¼Î± Î½Î± Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹ Ï„Î± Î¬Ï„Î¿Î¼Î± Î¼Îµ Ï…ÏˆÎ·Î»ÏŒ ÎµÎ¹ÏƒÏŒÎ´Î·Î¼Î±.

# ğŸ”¹ Support Vector Machine (SVM)
# True Positives (TP) = 1693 (ÎºÎ±Î»ÏÏ„ÎµÏÎ· Î±Ï€ÏŒÎ´Î¿ÏƒÎ· ÏƒÏ„Î¿ >50K)
# False Negatives (FN) = 0 (ÎºÎ±Î¼Î¯Î± ÏƒÏ‰ÏƒÏ„Î® Ï€ÏÏŒÎ²Î»ÎµÏˆÎ· Î³Î¹Î± >50K)
# Accuracy = 63.5% (Î¼Î­Ï„ÏÎ¹Î¿)
# ğŸ“Œ Î£Ï…Î¼Ï€Î­ÏÎ±ÏƒÎ¼Î±: Î¤Î¿ SVM Î±Ï€Î¿Î´Î¯Î´ÎµÎ¹ Î»Î¯Î³Î¿ ÎºÎ±Î»ÏÏ„ÎµÏÎ± Î±Ï€ÏŒ Ï„Î¿ Logistic Regression, Î±Î»Î»Î¬ Î´ÎµÎ½ Î´Î¹Î±Ï‡Ï‰ÏÎ¯Î¶ÎµÎ¹ ÎºÎ±Î»Î¬ Ï„Î¹Ï‚ Î´ÏÎ¿ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯ÎµÏ‚.

# ğŸ”¹ Gradient Boosting
# True Positives (TP) = 2 (Î¿Ï…ÏƒÎ¹Î±ÏƒÏ„Î¹ÎºÎ¬ Î±Ï€Î¿Ï„Ï…Î³Ï‡Î¬Î½ÎµÎ¹ Î½Î± Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹ Ï„Î·Î½ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î± >50K)
# False Negatives (FN) = 0
# Accuracy = 75.7% (Ï…ÏˆÎ·Î»ÏŒ)
# ğŸ“Œ Î£Ï…Î¼Ï€Î­ÏÎ±ÏƒÎ¼Î±: Î Î±ÏÏŒÎ»Î¿ Ï€Î¿Ï… Î­Ï‡ÎµÎ¹ Ï„Î¿ Ï…ÏˆÎ·Î»ÏŒÏ„ÎµÏÎ¿ Accuracy, Î´ÎµÎ½ Ï€ÏÎ¿Î²Î»Î­Ï€ÎµÎ¹ ÏƒÏ‡ÎµÎ´ÏŒÎ½ ÎºÎ±Î¸ÏŒÎ»Î¿Ï… ÏƒÏ‰ÏƒÏ„Î¬ Ï„Î·Î½ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î± >50K, Ï€Î¿Ï… ÎµÎ¯Î½Î±Î¹ Î¼ÎµÎ³Î¬Î»Î¿ Ï€ÏÏŒÎ²Î»Î·Î¼Î±


from imblearn.over_sampling import SMOTE
import sklearn
import imblearn

# Î•Ï†Î±ÏÎ¼Î¿Î³Î® SMOTE Î³Î¹Î± Ï„Î·Î½ ÎµÎ¾Î¹ÏƒÎ¿ÏÏÏŒÏ€Î·ÏƒÎ· Ï„Ï‰Î½ ÎºÎ»Î¬ÏƒÎµÏ‰Î½
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# Î•Ï€Î¹Î²ÎµÎ²Î±Î¯Ï‰ÏƒÎ· Ï„Î·Ï‚ Î½Î­Î±Ï‚ ÎºÎ±Ï„Î±Î½Î¿Î¼Î®Ï‚ Ï„Ï‰Î½ ÎºÎ»Î¬ÏƒÎµÏ‰Î½
unique, counts = np.unique(y_resampled, return_counts=True)
balanced_class_distribution = dict(zip(unique, counts))

# Î•Ï€Î±Î½ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· Ï„Ï‰Î½ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ Î¼Îµ Ï„Î± ÎµÎ¾Î¹ÏƒÎ¿ÏÏÎ¿Ï€Î·Î¼Î­Î½Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î±
new_results = []

for model_name, model in models.items():
    # Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· Î¼Îµ Ï„Î± Î½Î­Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î±
    model.fit(X_resampled, y_resampled)
    
    # Î ÏÏŒÎ²Î»ÎµÏˆÎ· ÏƒÏ„Î¿ test set
    y_pred_smote = model.predict(X_test)
    
    # Î‘Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ· Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½
    accuracy = accuracy_score(y_test, y_pred_smote)
    precision = classification_report(y_test, y_pred_smote, output_dict=True)["weighted avg"]["precision"]
    recall = classification_report(y_test, y_pred_smote, output_dict=True)["weighted avg"]["recall"]
    f1 = classification_report(y_test, y_pred_smote, output_dict=True)["weighted avg"]["f1-score"]
    
    # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½
    new_results.append({"Model": model_name, "Accuracy": accuracy, "Precision": precision, "Recall": recall, "F1-Score": f1})
    
    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred_smote)
    plt.figure(figsize=(5,4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=["<=50K", ">50K"], yticklabels=["<=50K", ">50K"])
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title(f"Confusion Matrix - {model_name} (SMOTE Applied)")
    plt.show()

# Î ÏÎ¿Î²Î¿Î»Î® Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½ Î¼ÎµÏ„Î¬ Ï„Î·Î½ ÎµÏ†Î±ÏÎ¼Î¿Î³Î® SMOTE
new_machine_learing_results_df = pd.DataFrame(new_results)
print(new_machine_learing_results_df)

# 1ï¸âƒ£ Logistic Regression (Î Î±Î»Î¹Î½Î´ÏÏŒÎ¼Î·ÏƒÎ·)
# ğŸ“Š Confusion Matrix

# 3721 ÏƒÏ‰ÏƒÏ„Î­Ï‚ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ Î³Î¹Î± <=50K (True Negatives)
# 3252 Î»Î±Î½Î¸Î±ÏƒÎ¼Î­Î½ÎµÏ‚ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ ÏŒÏ€Î¿Ï… <=50K Ï€ÏÎ¿Î²Î»Î­Ï†Î¸Î·ÎºÎ±Î½ Ï‰Ï‚ >50K
# 0 ÏƒÏ‰ÏƒÏ„Î­Ï‚ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ Î³Î¹Î± >50K (False Negatives)
# Î— Î±ÎºÏÎ¯Î²ÎµÎ¹Î± Î­Ï€ÎµÏƒÎµ ÏƒÏ„Î¿ 51.9%
# ğŸ” Î‘Î½Î¬Î»Ï…ÏƒÎ·

# ÎŸ Ï„Î±Î¾Î¹Î½Î¿Î¼Î·Ï„Î®Ï‚ ÎºÎ¬Î½ÎµÎ¹ Ï€Î¬ÏÎ± Ï€Î¿Î»Î»Î¬ Î»Î¬Î¸Î· ÏƒÏ„Î·Î½ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î± >50K.
# Î Î¹Î¸Î±Î½ÏŒÏ„Î±Ï„Î± Î´ÎµÎ½ Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Ï€ÏÎ¿ÏƒÎ±ÏÎ¼Î¿ÏƒÏ„ÎµÎ¯ ÏƒÏ‰ÏƒÏ„Î¬ ÏƒÏ„Î± Î½Î­Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Ï€Î¿Ï… Î´Î·Î¼Î¹Î¿ÏÏÎ³Î·ÏƒÎµ Ï„Î¿ SMOTE.
# Î“ÎµÎ½Î¹ÎºÎ¬, Î· Logistic Regression Î´ÎµÎ½ ÎµÎ¯Î½Î±Î¹ Ï„ÏŒÏƒÎ¿ Î¹ÏƒÏ‡Ï…ÏÏŒÏ‚ Ï„Î±Î¾Î¹Î½Î¿Î¼Î·Ï„Î®Ï‚ Î³Î¹Î± Ï€ÎµÏÎ¯Ï€Î»Î¿ÎºÎ± Ï€ÏÎ¿Î²Î»Î®Î¼Î±Ï„Î±.
# ğŸ›  Î¤Î¹ Î¼Ï€Î¿ÏÎ¿ÏÎ¼Îµ Î½Î± ÎºÎ¬Î½Î¿Ï…Î¼Îµ;

# Î”Î¿ÎºÎ¹Î¼Î® Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÏÎ½ ÎºÎ±Î½Î¿Î½Î¹ÎºÎ¿Ï€Î¿Î¹Î®ÏƒÎµÏ‰Î½ Î® Î´Î¹Î±Î³ÏÎ±Ï†Î® Ï€ÎµÏÎ¹Ï„Ï„ÏÎ½ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½.
# 2ï¸âƒ£ Random Forest
# ğŸ“Š Confusion Matrix

# 5542 ÏƒÏ‰ÏƒÏ„Î­Ï‚ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ Î³Î¹Î± <=50K
# 1431 Î»Î±Î½Î¸Î±ÏƒÎ¼Î­Î½ÎµÏ‚ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ Î³Î¹Î± <=50K
# Î— Î±ÎºÏÎ¯Î²ÎµÎ¹Î± Î²ÎµÎ»Ï„Î¹ÏÎ¸Î·ÎºÎµ ÏƒÏ„Î¿ 65.4% (ÏƒÏ…Î³ÎºÏÎ¹Ï„Î¹ÎºÎ¬ Î¼Îµ Ï€ÏÎ¹Î½)
# ğŸ” Î‘Î½Î¬Î»Ï…ÏƒÎ·

# Î¤Î¿ Random Forest Î±Î½Ï„Î±Ï€Î¿ÎºÏÎ¯Î¸Î·ÎºÎµ Ï€Î¿Î»Ï ÎºÎ±Î»ÏÏ„ÎµÏÎ± Î±Ï€ÏŒ Ï„Î· Logistic Regression.
# ÎˆÏ‡ÎµÎ¹ Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ· Î¹ÏƒÎ¿ÏÏÎ¿Ï€Î¯Î± ÏƒÏ„Î¹Ï‚ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚, Î±Î»Î»Î¬ Î±ÎºÏŒÎ¼Î± Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î»Î¬Î¸Î·.
# ğŸ›  Î¤Î¹ Î¼Ï€Î¿ÏÎ¿ÏÎ¼Îµ Î½Î± ÎºÎ¬Î½Î¿Ï…Î¼Îµ;

# Î‘ÏÎ¾Î·ÏƒÎ· Ï„Î¿Ï… Î±ÏÎ¹Î¸Î¼Î¿Ï Ï„Ï‰Î½ Î´Î­Î½Ï„ÏÏ‰Î½ (n_estimators).
# Î”Î¿ÎºÎ¹Î¼Î® Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÏÎ½ Ï…Ï€ÎµÏÏ€Î±ÏÎ±Î¼Î­Ï„ÏÏ‰Î½ (max_depth, min_samples_split, ÎºÏ„Î».).
# 3ï¸âƒ£ Support Vector Machine
# ğŸ“Š Confusion Matrix

# 4227 ÏƒÏ‰ÏƒÏ„Î­Ï‚ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ Î³Î¹Î± <=50K
# 2746 Î»Î±Î½Î¸Î±ÏƒÎ¼Î­Î½ÎµÏ‚ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ Î³Î¹Î± <=50K
# Î‘ÎºÏÎ¯Î²ÎµÎ¹Î± ÏƒÏ„Î¿ 55.2%
# ğŸ” Î‘Î½Î¬Î»Ï…ÏƒÎ·

# Î‘ÎºÏŒÎ¼Î± Ï€Î¿Î»Î»Î¬ Î»Î¬Î¸Î· ÎºÎ±Î¹ ÏƒÏ„Î¹Ï‚ Î´ÏÎ¿ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯ÎµÏ‚.
# ÎŸ Î±Î»Î³ÏŒÏÎ¹Î¸Î¼Î¿Ï‚ Î´ÎµÎ½ Î±Î½Ï„Î±Ï€Î¿ÎºÏÎ¯Î¸Î·ÎºÎµ ÎºÎ±Î»Î¬ ÏƒÏ„Î¿ SMOTE.
# ğŸ›  Î¤Î¹ Î¼Ï€Î¿ÏÎ¿ÏÎ¼Îµ Î½Î± ÎºÎ¬Î½Î¿Ï…Î¼Îµ;

# Î”Î¿ÎºÎ¹Î¼Î® Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÏÎ½ kernels (RBF, polynomial).
# Î¡ÏÎ¸Î¼Î¹ÏƒÎ· Ï„Î·Ï‚ Ï€Î±ÏÎ±Î¼Î­Ï„ÏÎ¿Ï… C.
# 4ï¸âƒ£ Gradient Boosting
# ğŸ“Š Confusion Matrix

# 6362 ÏƒÏ‰ÏƒÏ„Î­Ï‚ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ Î³Î¹Î± <=50K
# 611 Î»Î±Î½Î¸Î±ÏƒÎ¼Î­Î½ÎµÏ‚ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ Î³Î¹Î± <=50K
# Î— ÎºÎ±Î»ÏÏ„ÎµÏÎ· Î±ÎºÏÎ¯Î²ÎµÎ¹Î± ÏƒÏ„Î¿ 71.1%!
# ğŸ” Î‘Î½Î¬Î»Ï…ÏƒÎ·

# Î¤Î¿ Gradient Boosting Î±Ï€Î¿Î´Î¯Î´ÎµÎ¹ Ï„Î¿ ÎºÎ±Î»ÏÏ„ÎµÏÎ¿ Î±Ï€Î¿Ï„Î­Î»ÎµÏƒÎ¼Î±.
# ÎˆÏ‡ÎµÎ¹ Ï„Î·Î½ Ï…ÏˆÎ·Î»ÏŒÏ„ÎµÏÎ· Î±ÎºÏÎ¯Î²ÎµÎ¹Î±, recall ÎºÎ±Î¹ F1-score.
# ğŸ›  Î¤Î¹ Î¼Ï€Î¿ÏÎ¿ÏÎ¼Îµ Î½Î± ÎºÎ¬Î½Î¿Ï…Î¼Îµ;

# Î‘ÏÎ¾Î·ÏƒÎ· Ï„Ï‰Î½ n_estimators.
# Î”Î¿ÎºÎ¹Î¼Î® Learning Rate (Î¼ÎµÎ¯Ï‰ÏƒÎ· Î³Î¹Î± ÎºÎ±Î»ÏÏ„ÎµÏÎ· Î³ÎµÎ½Î¯ÎºÎµÏ…ÏƒÎ·)
####Î²Î»ÎµÏ‚Ï€Î¿Ï…Î¼Îµ Î¿Ï„Î¹ Î¿Ï…Ï„Îµ Î¼Îµ Ï„Î·Î½ ÎµÎ¾ÏƒÎ¹ÏƒÎ¿ÏÎ¿Ï€Î·ÏƒÎ· Î´ÎµÎ´Î¿Î¼ÎµÎ½Ï‰Î½ Î²Î³Î±Î¶Î¿Ï…Î¼Îµ ÎºÎ±ÏÎ· Î¿Ï€Î¿Ï„Îµ Ï€Î±Î¼Îµ Î½Î± Î´Î¿ÎºÎ¹Î¼Î±ÏƒÎ¿Ï…Î¼Îµ Î¼Î¿Î½Î¿ Ï„Î¹Ï‚ 5 Î¼ÎµÏ„Î±Î²Î»Î·Ï„ÎµÏ‚ Î±Ï€Î¿ Ï„Î¿ select k best
selected_features_kbest = ["education-num", "marital-status", "race", "capital-gain", "native-country"]
X_selected = other_variables[selected_features_kbest]
X_train, X_test, y_train, y_test = train_test_split(X_selected, income.values.ravel(), test_size=0.2, random_state=42)

models = {
    "Logistic Regression": LogisticRegression(max_iter=500),
    "Random Forest": RandomForestClassifier(n_estimators=300, random_state=42),
    "Support Vector Machine": SVC(),
    "Gradient Boosting": GradientBoostingClassifier(n_estimators=100, random_state=42)
}

results = []

for model_name, model in models.items():
    # Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· Î¼Îµ Ï„Î± Î½Î­Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î±
    model.fit(X_train, y_train)
    
    # Î ÏÏŒÎ²Î»ÎµÏˆÎ· ÏƒÏ„Î¿ test set
    y_pred = model.predict(X_test)
    
    # Î‘Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ· Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½
    accuracy = accuracy_score(y_test, y_pred)
    precision = classification_report(y_test, y_pred, output_dict=True)["weighted avg"]["precision"]
    recall = classification_report(y_test, y_pred, output_dict=True)["weighted avg"]["recall"]
    f1 = classification_report(y_test, y_pred, output_dict=True)["weighted avg"]["f1-score"]
    
    # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½
    results.append({"Model": model_name, "Accuracy": accuracy, "Precision": precision, "Recall": recall, "F1-Score": f1})
    
    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(5,4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=["<=50K", ">50K"], yticklabels=["<=50K", ">50K"])
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title(f"Confusion Matrix - {model_name} (5best)")
    plt.show()

# Î ÏÎ¿Î²Î¿Î»Î® Ï„Ï‰Î½ Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½
results_df_5best = pd.DataFrame(results)
print(results_df_5best)
#oute edw vriskoume kapoio montelo pou na einai deleastiko
# 1ï¸âƒ£ Feature Selection with SelectKBest
# You used SelectKBest to reduce the number of features to the top 5 most relevant based on their correlation with the target.
# The models trained on these selected features show an accuracy comparable to those trained on the full dataset.
# However, the confusion matrices indicate that some models perform extremely poorly in predicting the ">50K" class.
# 2ï¸âƒ£ Confusion Matrices (5 Best Features)
# Logistic Regression, SVM: These models completely fail to predict the ">50K" class (no values in the true positive quadrant).
# Random Forest & Gradient Boosting: Show minor improvements in detecting the ">50K" class, but the number of false negatives remains high.
# ğŸ”´ Interpretation:

# The drastic class imbalance is likely causing these models to be biased towards predicting only "<=50K".
# The removal of important features might have stripped away crucial information needed to separate the two income classes effectively.
#  Observations:

# Accuracy is not significantly affected by the feature reduction.
# Gradient Boosting shows the highest Precision (0.677), indicating it is more confident in its positive predictions.
# Random Forest shows similar performance to the full-feature model, meaning it still retains good predictive power.

#ÏƒÏ…Î½Î¿Î»Î¹ÎºÎ¿ ÏƒÏ…Î¼Ï€ÎµÏÎ±ÏƒÎ¼Î± 
# ÎœÎµ Î²Î¬ÏƒÎ· Ï„Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± Ï„Ï‰Î½ Ï„ÏÎ¹ÏÎ½ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÏÎ½ Ï€ÏÎ¿ÏƒÎµÎ³Î³Î¯ÏƒÎµÏ‰Î½ (Î±ÏÏ‡Î¹ÎºÎ¬ Î´ÎµÎ´Î¿Î¼Î­Î½Î±, ÎµÎ¾Î¹ÏƒÎ¿ÏÏÎ¿Ï€Î·Î¼Î­Î½Î± Î¼Îµ SMOTE, ÎºÎ±Î¹ Ï‡ÏÎ®ÏƒÎ· Ï„Ï‰Î½ 5 ÎºÎ±Î»ÏÏ„ÎµÏÏ‰Î½ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½), Î· ÎºÎ±Î»ÏÏ„ÎµÏÎ· ÎµÏ€Î¹Î»Î¿Î³Î® ÎµÎ¾Î±ÏÏ„Î¬Ï„Î±Î¹ Î±Ï€ÏŒ Ï„Î·Î½ Ï€ÏÎ¿Ï„ÎµÏÎ±Î¹ÏŒÏ„Î·Ï„Î¬ ÏƒÎ¿Ï….

# Î‘Î½ Î´Î¯Î½Î¿Ï…Î¼Îµ Î­Î¼Ï†Î±ÏƒÎ· ÏƒÏ„Î·Î½ Î±ÎºÏÎ¯Î²ÎµÎ¹Î± (Accuracy):

# Gradient Boosting ÏƒÏ„Î¹Ï‚ Î±ÏÏ‡Î¹ÎºÎ­Ï‚ ÏƒÏ…Î½Î¸Î®ÎºÎµÏ‚ ÎºÎ±Î¹ Î¼Îµ Ï„Î± 5 ÎºÎ±Î»ÏÏ„ÎµÏÎ± Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ Î­Ï‡ÎµÎ¹ Ï„Î·Î½ Ï…ÏˆÎ·Î»ÏŒÏ„ÎµÏÎ· Î±ÎºÏÎ¯Î²ÎµÎ¹Î± (0.757).
# Random Forest ÎµÏ€Î¯ÏƒÎ·Ï‚ Î­Ï‡ÎµÎ¹ ÎºÎ±Î»Î® Î±ÎºÏÎ¯Î²ÎµÎ¹Î±, Î±Î»Î»Î¬ Ï‡Î±Î¼Î·Î»ÏŒÏ„ÎµÏÎ· Î±Ï€ÏŒ Ï„Î¿ Gradient Boosting.
# Î‘Î½ Î´Î¯Î½Î¿Ï…Î¼Îµ Î­Î¼Ï†Î±ÏƒÎ· ÏƒÏ„Î·Î½ Î±Î½Î¬ÎºÎ»Î·ÏƒÎ· (Recall) Î³Î¹Î± Ï„Î·Î½ Î±Î½Î¯Ï‡Î½ÎµÏ…ÏƒÎ· Ï„Ï‰Î½ Ï…ÏˆÎ·Î»ÏÎ½ ÎµÎ¹ÏƒÎ¿Î´Î·Î¼Î¬Ï„Ï‰Î½ (>50K):

# ÎœÎµÏ„Î¬ Ï„Î·Î½ ÎµÏ†Î±ÏÎ¼Î¿Î³Î® Ï„Î¿Ï… SMOTE, Ï„Î¿ Logistic Regression ÎºÎ±Î¹ Ï„Î¿ Random Forest Î­Ï‡Î¿Ï…Î½ ÎºÎ±Î»ÏÏ„ÎµÏÎ· Î¹ÏƒÎ¿ÏÏÎ¿Ï€Î¯Î± recall, Î±Î»Î»Î¬ Î¼ÎµÎ¹Ï‰Î¼Î­Î½Î· Î±ÎºÏÎ¯Î²ÎµÎ¹Î±.
# Î‘Î½ Î´Î¯Î½Î¿Ï…Î¼Îµ Î­Î¼Ï†Î±ÏƒÎ· ÏƒÏ„Î·Î½ Î¹ÏƒÎ¿ÏÏÎ¿Ï€Î¯Î± Î¼ÎµÏ„Î±Î¾Ï Î±ÎºÏÎ¯Î²ÎµÎ¹Î±Ï‚ ÎºÎ±Î¹ Î±Î½Î¬ÎºÎ»Î·ÏƒÎ·Ï‚ (F1-score):

# Gradient Boosting Î´Î¹Î±Ï„Î·ÏÎµÎ¯ Ï„Î·Î½ Ï…ÏˆÎ·Î»ÏŒÏ„ÎµÏÎ· F1-score ÏƒÏ„Î¹Ï‚ Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎµÏ‚ Ï€ÎµÏÎ¹Ï€Ï„ÏÏƒÎµÎ¹Ï‚.
# Î¤ÎµÎ»Î¹ÎºÎ® ÎµÏ€Î¹Î»Î¿Î³Î®:
# Gradient Boosting Ï‡Ï‰ÏÎ¯Ï‚ SMOTE ÎºÎ±Î¹ Ï‡Ï‰ÏÎ¯Ï‚ Ï„Î· Î¼ÎµÎ¯Ï‰ÏƒÎ· Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½ (Î¼Îµ ÏŒÎ»Î± Ï„Î± features).

# ÎˆÏ‡ÎµÎ¹ Ï„Î·Î½ ÎºÎ±Î»ÏÏ„ÎµÏÎ· ÏƒÏ…Î½Î¿Î»Î¹ÎºÎ® Î±Ï€ÏŒÎ´Î¿ÏƒÎ·, Î¼Îµ Ï…ÏˆÎ·Î»Î® Î±ÎºÏÎ¯Î²ÎµÎ¹Î±, recall, precision ÎºÎ±Î¹ F1-score.
# Î”ÎµÎ½ Ï‡Î¬Î½ÎµÎ¹ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯Î± Î±Ï€ÏŒ Ï„Î¿ dataset.
# Î‘Î½ ÏŒÎ¼Ï‰Ï‚ Î´Î¯Î½ÎµÎ¹Ï‚ Î­Î¼Ï†Î±ÏƒÎ· ÏƒÏ„Î¿ Î½Î± Î¼ÎµÎ¹ÏÏƒÎµÎ¹Ï‚ Ï„Î· Î´Î¹Î¬ÏƒÏ„Î±ÏƒÎ· Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ ÎºÎ±Î¹ Î½Î± Î²ÎµÎ»Ï„Î¹ÏÏƒÎµÎ¹Ï‚ Ï„Î·Î½ Ï„Î±Ï‡ÏÏ„Î·Ï„Î±, Ï„ÏŒÏ„Îµ Î· ÎµÏ€Î¹Î»Î¿Î³Î® Ï„Î¿Ï… Random Forest Î¼Îµ Ï„Î± 5 ÎºÎ±Î»ÏÏ„ÎµÏÎ± Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ ÎµÎ¯Î½Î±Î¹ ÎºÎ±Î»Î® ÎµÎ½Î±Î»Î»Î±ÎºÏ„Î¹ÎºÎ®, ÎºÎ±Î¸ÏÏ‚ ÎºÏÎ±Ï„Î¬ÎµÎ¹ Î±Î¾Î¹Î¿Ï€ÏÎµÏ€Î® Î±ÎºÏÎ¯Î²ÎµÎ¹Î± Ï‡Ï‰ÏÎ¯Ï‚ Ï€ÎµÏÎ¹Ï„Ï„Î¬ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬.
#Ï€Î±Î¼Îµ Ï„Ï‰ÏÎ± Î¼Îµ Ï„Î¿ gradient boosting Î½Î± Ï€ÏÎ¿ÏƒÏ€Î±Î¸Î·ÏƒÎ¿Ï…Î¼Îµ Î½Î± Î¿ Î²ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¹Î·ÏƒÎ¿Ï…Î¼Îµ
from sklearn.model_selection import GridSearchCV
X = other_variables.copy()  # Features
y = income.values.ravel()  # Target variable

# Split dataset into 80% training and 20% testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ÎŸÏÎ¹ÏƒÎ¼ÏŒÏ‚ Ï„Î¿Ï… Gradient Boosting Classifier
gb = GradientBoostingClassifier(random_state=42)

# ÎŸÏÎ¹ÏƒÎ¼ÏŒÏ‚ Ï„Î¿Ï… grid Î³Î¹Î± hyperparameter tuning
param_grid = {
    'n_estimators': [100, 300, 500],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10],
    'subsample': [0.8, 1.0]
}

# GridSearchCV Î³Î¹Î± Ï„Î·Î½ ÎµÏÏÎµÏƒÎ· Ï„Ï‰Î½ ÎºÎ±Î»ÏÏ„ÎµÏÏ‰Î½ Ï€Î±ÏÎ±Î¼Î­Ï„ÏÏ‰Î½
grid_search = GridSearchCV(gb, param_grid, cv=3, scoring='f1', n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)

# ÎšÎ±Î»ÏÏ„ÎµÏÎµÏ‚ Ï€Î±ÏÎ¬Î¼ÎµÏ„ÏÎ¿Î¹
best_params = grid_search.best_params_
best_params
# {'learning_rate': 0.2,
#  'max_depth': 7,
#  'min_samples_split': 10,
#  'n_estimators': 500,
#  'subsample': 0.8}

#Î´Î¿ÎºÎ¹Î¼Î· Ï„Î½Ï‰ ÎºÎ±Î»Ï…ÎµÏÏ‰Î½ Ï€Î±ÏÎ±Î¼ÎµÏ„ÏÏ‰Î½ 
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
best_gb = GradientBoostingClassifier(
    learning_rate=0.2,
    max_depth=7,
    min_samples_split=10,
    n_estimators=500,
    subsample=0.8,
    random_state=42
)

# Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· Ï„Î¿Ï… Î²ÎµÎ»Ï„Î¹Ï‰Î¼Î­Î½Î¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï…
best_gb.fit(X_train, y_train)

# Î ÏÏŒÎ²Î»ÎµÏˆÎ· ÏƒÏ„Î¿ test set
y_pred_gb = best_gb.predict(X_test)

# Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ Î¼ÎµÏ„ÏÎ¹ÎºÏÎ½ Î±Ï€ÏŒÎ´Î¿ÏƒÎ·Ï‚
accuracy = accuracy_score(y_test, y_pred_gb)
precision = precision_score(y_test, y_pred_gb)
recall = recall_score(y_test, y_pred_gb)
f1 = f1_score(y_test, y_pred_gb)

# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Ï„Ï‰Î½ Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½
print(f"Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î¿ Gradient Boosting Performance:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")

# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Confusion Matrix
cm = confusion_matrix(y_test, y_pred_gb)
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=["<=50K", ">50K"], yticklabels=["<=50K", ">50K"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title(f"Confusion Matrix - Optimized Gradient Boosting")
plt.show()
#done with 4 ,no best params first gradient boosting correct




